[
  {
    "objectID": "docs/developer/tutorial-data-management/slides/index.html",
    "href": "docs/developer/tutorial-data-management/slides/index.html",
    "title": "Slides training on spatial data flows",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 15, 2023\n\n\n1.1 Training Overview\n\n\nPaul van Genuchten\n\n\n\n\nSep 15, 2023\n\n\n1.2 FAIR data\n\n\nPaul van Genuchten\n\n\n\n\nMay 9, 2023\n\n\n1.3 Tools\n\n\nPaul van Genuchten\n\n\n\n\nMay 9, 2023\n\n\n2.1 Collect and Publish metadata\n\n\nPaul van Genuchten\n\n\n\n\nMay 9, 2023\n\n\n3.1 Data services\n\n\nPaul van Genuchten\n\n\n\n\nMay 9, 2023\n\n\n3.2 Service Quality\n\n\nPaul van Genuchten\n\n\n\n\nMay 9, 2023\n\n\n4.1 Soil Profile Datamanagement\n\n\nPaul van Genuchten\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.2-service-quality.html#service-quality",
    "href": "docs/developer/tutorial-data-management/slides/3.2-service-quality.html#service-quality",
    "title": "3.2 Service Quality",
    "section": "Service quality",
    "text": "Service quality\n\nMonitor the quality of data services is a driver for improvement\nCreate a monthly report with various statistics"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.2-service-quality.html#usage",
    "href": "docs/developer/tutorial-data-management/slides/3.2-service-quality.html#usage",
    "title": "3.2 Service Quality",
    "section": "Usage",
    "text": "Usage\n\nAggregate and display access logs\nHow are people using the services\nWhich aspects are popular\nWhich components have a bad performance"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.2-service-quality.html#availability",
    "href": "docs/developer/tutorial-data-management/slides/3.2-service-quality.html#availability",
    "title": "3.2 Service Quality",
    "section": "Availability",
    "text": "Availability\n\nHow often are our services available\nReceive a notification if a service is not available"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.2-service-quality.html#complience",
    "href": "docs/developer/tutorial-data-management/slides/3.2-service-quality.html#complience",
    "title": "3.2 Service Quality",
    "section": "Complience",
    "text": "Complience\n\nDo services comply with the indicated standards (wms, wfs)"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.2-service-quality.html#security-incidents-log",
    "href": "docs/developer/tutorial-data-management/slides/3.2-service-quality.html#security-incidents-log",
    "title": "3.2 Service Quality",
    "section": "Security incidents log",
    "text": "Security incidents log\n\nFraudulent email received\nMany requests from certain IP\nLow quality password detected\nBackup restored due to failing disk"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.2-service-quality.html#search-engine-optimisation-sso",
    "href": "docs/developer/tutorial-data-management/slides/3.2-service-quality.html#search-engine-optimisation-sso",
    "title": "3.2 Service Quality",
    "section": "Search engine optimisation (SSO)",
    "text": "Search engine optimisation (SSO)\n\nCan users find our materials on Search Engines?\nGoogle/Bing offers a SSO Console where you can view crawling and usage statistics"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.2-service-quality.html#tools",
    "href": "docs/developer/tutorial-data-management/slides/3.2-service-quality.html#tools",
    "title": "3.2 Service Quality",
    "section": "Tools",
    "text": "Tools\n\nCheck local IT to understand which tools are offered\nGeohealthcheck is a tool to check availability and complience\nAWStats is a basic tool to evaluate access logs\npingdom & uptimerobot are saas providers offering availability testing\n\n\n\n\n\nWorkshop on effective spatial data management"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/2.1-collect-publish-metadata.html#organisations-organise-their-data-in-file-repositories-or-databases",
    "href": "docs/developer/tutorial-data-management/slides/2.1-collect-publish-metadata.html#organisations-organise-their-data-in-file-repositories-or-databases",
    "title": "2.1 Collect and Publish metadata",
    "section": "Organisations organise their data in file repositories or databases",
    "text": "Organisations organise their data in file repositories or databases\n\nFile hierarchy"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/2.1-collect-publish-metadata.html#a-readme-in-each-folder",
    "href": "docs/developer/tutorial-data-management/slides/2.1-collect-publish-metadata.html#a-readme-in-each-folder",
    "title": "2.1 Collect and Publish metadata",
    "section": "A readme in each folder",
    "text": "A readme in each folder\n\nAdd a readme to describe the resources in a folder\nWhy not structure the content in the readme, so also machines can read it?"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/2.1-collect-publish-metadata.html#metadata-control-file",
    "href": "docs/developer/tutorial-data-management/slides/2.1-collect-publish-metadata.html#metadata-control-file",
    "title": "2.1 Collect and Publish metadata",
    "section": "Metadata Control File",
    "text": "Metadata Control File\n\nA convention of the pygeometa community (Meteo Canada)\nYAML, easy to read by humans and machines\nA subset of the ISO19115 standard for metadata\n\n\nMCF"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/2.1-collect-publish-metadata.html#pygeometa-library",
    "href": "docs/developer/tutorial-data-management/slides/2.1-collect-publish-metadata.html#pygeometa-library",
    "title": "2.1 Collect and Publish metadata",
    "section": "pygeometa library",
    "text": "pygeometa library\n\na library to work with MCF\nexports MCF to various metadata models (iso, dcat, stac)\nhttps://github.com/geopython/pygeometa"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/2.1-collect-publish-metadata.html#the-geodatacrawler-tool",
    "href": "docs/developer/tutorial-data-management/slides/2.1-collect-publish-metadata.html#the-geodatacrawler-tool",
    "title": "2.1 Collect and Publish metadata",
    "section": "The geodatacrawler tool",
    "text": "The geodatacrawler tool\n\nDeveloped at ISRIC in the last year\nOn the shoulders of giants; GDAL, pygeometa, pandas\nGenerates MCF files for a file repository\nImports metadata from external sources\nExtracts MCF files from a repository to load it into a catalogue\nCreates map services configuration for files in the repository"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/2.1-collect-publish-metadata.html#pycsw-catalogue",
    "href": "docs/developer/tutorial-data-management/slides/2.1-collect-publish-metadata.html#pycsw-catalogue",
    "title": "2.1 Collect and Publish metadata",
    "section": "pycsw catalogue",
    "text": "pycsw catalogue\n\nA catalogue implementation in python\nLimited funtionality, easy maintenance\nWide range of supported catalogue standards, serves many communities\nDatabase backend\nSkin customisable with jinja templates\n\n\n\n\n\nWorkshop on effective spatial data management"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#persistent-identification",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#persistent-identification",
    "title": "1.2 FAIR data",
    "section": "Persistent identification",
    "text": "Persistent identification\nPersistent identification, for continued findability\n\nConsider that a proper id can outlive a project (or organisation)\nChoice of domain and path (owned, authoritative, neutral, prevent names)\nSet up an identification proxy (doi.org/w3id.org)"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#metadata",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#metadata",
    "title": "1.2 FAIR data",
    "section": "Metadata",
    "text": "Metadata\n\nMetadata describes title, abstract, author of a resource\nFacilitate findability and understand if a resource is relevant\nCan organize resources in groups by tagging"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#standards-for-metadata-exchange",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#standards-for-metadata-exchange",
    "title": "1.2 FAIR data",
    "section": "Standards for metadata exchange",
    "text": "Standards for metadata exchange\n\nFacilitate the exchange of resources between communities\nProtocols to exchange metadata:\n\n\n\n\nCommunity\nStandard\nProtocol\n\n\n\n\nOpen data/Sematic web\nDCAT\nSPARQL\n\n\nScience\nDatacite\nOAI-PMH\n\n\nGeospatial\niso19115\nCSW\n\n\nEarth observation\nSTAC\nSTAC\n\n\nSearch engines\nSchema.org\njson-ld/microdata\n\n\nEcology\nEML"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#catalogue",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#catalogue",
    "title": "1.2 FAIR data",
    "section": "Catalogue",
    "text": "Catalogue\n\nRecords are brought into a catalogue, where they can be searched and assessed\nCatalogues can exchange records to increase discoverability\nCatalogues can cross borders between communities by transforming metadata to relevant community standards and protocols"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#search-engines",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#search-engines",
    "title": "1.2 FAIR data",
    "section": "Search engines",
    "text": "Search engines\n\nSearch engines crawl the content of catalogues\nIf a catalogue supports schema.org annotations, the content can also be extracted in a structured way\nExample"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#persistence",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#persistence",
    "title": "1.2 FAIR data",
    "section": "Persistence",
    "text": "Persistence\n\nMove the resource to a shared environment (backup)\nConsider a URL strategy\nUse a facade identifier (DOI)"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#data-lifecycle",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#data-lifecycle",
    "title": "1.2 FAIR data",
    "section": "Data lifecycle",
    "text": "Data lifecycle\n\nConsider upfront when to remove a resource (10 yrs?)\nWhat happens to the URI of a resource which is archived?\nMetadata should stay available, even if the data are no longer"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#repository-software",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#repository-software",
    "title": "1.2 FAIR data",
    "section": "Repository software",
    "text": "Repository software\n\nWebdav (or webserver software)\nZenodo, Dataverse\nDocument Management Systems (DMS)\nCloud storage (google drive, dropbox, Amazon, Sharepoint)"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#testing-tools",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#testing-tools",
    "title": "1.2 FAIR data",
    "section": "testing tools",
    "text": "testing tools\n\nAutomated link checking\nUsage logs filter on status 404 & referer\nGoogle Search Console notifies broken links"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#universal-formats",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#universal-formats",
    "title": "1.2 FAIR data",
    "section": "Universal formats",
    "text": "Universal formats\nFacilitates accessing a resource with commonly available tooling, or be refactored if a software is abandoned\n\nProprietary vs Open (eg. ecw vs tiff)\nDe facto vs Formalised (eg. YAML vs XML)\nBinary vs text based (eg. shapefile vs GeoJSON)\nCloud optimised vs Cloud native vs traditional (eg. COG vs GeoZarr vs GeoTiff)\nEmbedded metadata"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#adopt-common-vocabularies",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#adopt-common-vocabularies",
    "title": "1.2 FAIR data",
    "section": "Adopt common vocabularies",
    "text": "Adopt common vocabularies\nAdopting a standardised model enables aggregation of data.\n\nRelational models\nUML/GML models\nSemantic web ontologies"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.2-fair.html#relevant-vocabularies",
    "href": "docs/developer/tutorial-data-management/slides/1.2-fair.html#relevant-vocabularies",
    "title": "1.2 FAIR data",
    "section": "Relevant vocabularies",
    "text": "Relevant vocabularies\n\nISO28258 / INSPIRE / GLOSIS Web Ontology\nAgrovoc\nWRB / FAO Soil Classification Guidelines"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Land Soil Crop Hub",
    "section": "",
    "text": "This hub facilitates effective access to Land Soil and Crop information data, making existing resources better findable, accessible, interoperable and reusable (FAIR).\nThe LSC hub supports improved decision-making for climate-smart agriculture at national, regional and local levels. The focus is on two use cases: soil fertility management and soil water conservation."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Land Soil Crop Hub",
    "section": "Data",
    "text": "Data\nThe LSC hub offers a diverse range of datasets, maps and documents, including observational data from field studies (e.g., soil and water samples, crop yields), aerial and space observations, administrative boundaries, economic data (price development), predictive model outputs (soil, weather, yields), and population statistics.\n\nDatasets\nSearch datasets or browse by category.\n\n\n  \n  \n  \n\n\n\n\n Land\n\n Soil\n\n Crop\n\n Climate\n\n Water\n\n\n\n\n\n\n\nMaps\nAnnual cropland extent map for Central Africa produced by Digital Earth.\n\n\n\n\n\n\nOpen mapviewer\n\n\n\n\n\nDocuments\nRead more about the various APIs which are available for this service.\n\n\n\n\n\n\nView documents"
  },
  {
    "objectID": "index.html#information-services",
    "href": "index.html#information-services",
    "title": "Land Soil Crop Hub",
    "section": "Information services",
    "text": "Information services\nInformation services provide dedicated information derived from relevant data to a targeted audience.\nProcessing raw data in many cases requires expert knowledge. To bring the value of the data to a wider audience, it is of interest to set up information services derived from that data to targeted audiences.\nBrowse services Read more"
  },
  {
    "objectID": "index.html#predictive-modeling",
    "href": "index.html#predictive-modeling",
    "title": "Land Soil Crop Hub",
    "section": "Predictive modeling",
    "text": "Predictive modeling\nPredictive modeling in the LSC hub focuses on Soil Fertility and Soil Water Conservation and is a data-driven analytical approach that involves the use of statistical or machine learning techniques to create models that can make predictions about future events or outcomes based on historical data.\nBrowse models Read more"
  },
  {
    "objectID": "index.html#user-stories",
    "href": "index.html#user-stories",
    "title": "Land Soil Crop Hub",
    "section": "User stories",
    "text": "User stories\nThe two cases below are descriptions of key applications for which the LSC hub can be used, including the main stakeholders, the main issues and the models through which ‘LSC data’ is converted into information services that support informed decision making.\n\n\n\n\n\nUSE CASE\n\n\nSoil Fertility Management\nCurrent fertilizer and soil recommendations lack local context, leading to soil health decline and lower productivity. Integrated Soil Fertility Management can improve practices, boost yields, and provide climate benefits. This case aims to deliver better agronomic advice to farmers via agricultural extension services or directly, using existing data and tools.\n\nExplore this use case\n\n\n\n\n\n\n\n\nUSE CASE\n\n\nSoil Water Conservation\nSoil erosion threatens sustainability, climate, and food security in hilly regions of Ethiopia, Kenya, and Rwanda. Current land practices neglect local factors and erosion risks. The goal is to inform stakeholders and promote sustainable land practices for LDN, providing catchment managers and farmers with relevant information through the LSC-hub.\n\nExplore this use case"
  },
  {
    "objectID": "index.html#hub-community",
    "href": "index.html#hub-community",
    "title": "Land Soil Crop Hub",
    "section": "Hub community",
    "text": "Hub community\nThe heart of the knowledge hub. Here, you’ll find like-minded participants who share a passion for learning and knowledge exchange.\nJoin our diverse community of learners, experts, and enthusiasts to engage in discussions, share insights, and collaborate on soil fertility and soil water conservation topics. Explore, connect, contribute and make the hub grow.\n\nVisit community"
  },
  {
    "objectID": "index.html#join-our-newsletter",
    "href": "index.html#join-our-newsletter",
    "title": "Land Soil Crop Hub",
    "section": "Join our newsletter",
    "text": "Join our newsletter\nSubscribe to our newsletter and be the first to receive the latest data updates and community news about the Kenya Land Soil and Crop hub.\n\nSubscribe now"
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "LSC Hubs documentation",
    "section": "",
    "text": "Documentation of this hub has been split up in 3 groups, for users, for administrators and for developers.\n\n\n\nUser documentation\nAdministrator documentation\nDeveloper documentation"
  },
  {
    "objectID": "docs/index.html#contents",
    "href": "docs/index.html#contents",
    "title": "LSC Hubs documentation",
    "section": "",
    "text": "User documentation\nAdministrator documentation\nDeveloper documentation"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/9-advanced-options.html",
    "href": "docs/developer/tutorial-data-management/9-advanced-options.html",
    "title": "Advanced options",
    "section": "",
    "text": "Various extensions are possible to tailor the system to your organisation needs.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Advanced options"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/9-advanced-options.html#terriajs",
    "href": "docs/developer/tutorial-data-management/9-advanced-options.html#terriajs",
    "title": "Advanced options",
    "section": "TerriaJS",
    "text": "TerriaJS\nTerriaJS is a modern web gis application, which includes a widget to query a catalogue. From the catalogue search results the data can be added to the TerriaJS map.\nThe main docker image definition can be used to build and run terriaJS locally.\ngit clone https://github.com/TerriaJS/TerriaMap\ncd TerriaMap\ndocker build -t local/terria .\ndocker run -p 3001:3001 local/terria\nVisit http://localhost:3001 to see TerriaJS in action.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Advanced options"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/9-advanced-options.html#tailored-metadata-schema-in-mcf-pygeometa",
    "href": "docs/developer/tutorial-data-management/9-advanced-options.html#tailored-metadata-schema-in-mcf-pygeometa",
    "title": "Advanced options",
    "section": "Tailored metadata schema in mcf / pygeometa",
    "text": "Tailored metadata schema in mcf / pygeometa\nWhen using pygeometa to render a iso19139 document, you can use a tailered output schema, to match organisation needs (for example use a hardcoded publisher section).\nMCF allows to add any additional properties not listed on the json schema, which is a very easy way to extend the schema. However note that MCF validation may fail.\nGeoDataCrawler internally uses pygeometa to manage mcf, a location of a extended schema is one of the optional parameters to GeoDataCrawler.\nMDME does not use the mcf json schema as-is, because some of the mcf json schema elements are not supported by MDME. In case you extend the MCF schema and aim to use MDME, you need to also extend the MDME schema likewise.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Advanced options"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/9-advanced-options.html#publishing-extended-vocabularies",
    "href": "docs/developer/tutorial-data-management/9-advanced-options.html#publishing-extended-vocabularies",
    "title": "Advanced options",
    "section": "Publishing (extended) vocabularies",
    "text": "Publishing (extended) vocabularies\nIn some cases it is relevant as a national or regional coordinator to publish a (extended) codelist which provides concepts which are only relevant within your area. users in your area can then easily link to the concepts in that vocabulary and benefit from the regional interoperability.\nThe SKOS ontology is a commonly accepted mechanism to create vocabularies. It allows to link concepts to similar, wider or narrower concepts. When creating a new vocabulary you typically start with a list of concepts in Excel.\nThe Glosis web ontology contains a transformer script which convert a csv (exported from Excel) file to a skos document, alternatively you can use a tool such as skos play. The skos file can best be hosted on a git repository. Anybody can access it, and provide improvement suggestions.\nThe python vocview library is able to render a skos file to a nice web layout which is easy to browse and search through by humans. See a live implementation at tern.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Advanced options"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/9-advanced-options.html#summary",
    "href": "docs/developer/tutorial-data-management/9-advanced-options.html#summary",
    "title": "Advanced options",
    "section": "Summary",
    "text": "Summary\nWith this topic we conclude our training on data management. We hope you enjoyed the materials. Notice that the training can act as a starting point to a number of other resources. Let us know via Git issues if you have improvement suggestions for the materials.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Advanced options"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/7-providing-mapservices.html",
    "href": "docs/developer/tutorial-data-management/7-providing-mapservices.html",
    "title": "Providing convenience APIs",
    "section": "",
    "text": "For spatial datasets it is of interest to share them via convenience APIs, so the datasets can be downloaded in parts or easily be visualised in common tools such as QGIS, OpenLayers & Leaflet. The standards on data services of the Open Geospatial Consortium are designed with this purpose. These APIs give direct access to subsets or map visualisations of a dataset.\nIn this paragraph you will be introduced to various standardised APIs, after which we introduce an approach to publish datasets, which builds on the data management approach introduced in the previous paragraphs.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Providing convenience APIs"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/7-providing-mapservices.html#standardised-data-apis",
    "href": "docs/developer/tutorial-data-management/7-providing-mapservices.html#standardised-data-apis",
    "title": "Providing convenience APIs",
    "section": "Standardised data APIs",
    "text": "Standardised data APIs\nStandardised mapping APIs, such as Web Map Service (WMS), Web Feature service (WFS) and Web Coverage Service (WCS), originate from the beginning of this century. In recent years several challenges have been identified around these standards, which led to a series of Spatial data on the web best practices. OGC then initiated a new generation of standards based on these best practices.\nAn overview of both generations:\n\n\n\nOWS\nOGC-API\nDescription\n\n\n\n\nWeb Map Service (WMS)\nMaps\nProvides a visualisation of a subset of the data\n\n\nWeb Feature Service (WFS)\nFeatures\nAPI to request a subset of the vector features\n\n\nWeb Coverage Service (WCS)\nCoverages\nAPI to interact with grid sources\n\n\nSensor Observation Service (SOS)\nSensorthings\nRetrieve subsets of sensor observations\n\n\nWeb Processing Service (WPS)\nOGCAPI:Processes\nRun processes on data ]\n\n\nCatalogue Service for the web (CSW)\nOGCAPI:Records\nRetrieve and filter catalogue records\n\n\n\nNotice that most of the mapping software supports the standards of both generations. However, due to their recent introduction, expect incidental challenges in the implementations of OGC APIs.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Providing convenience APIs"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/7-providing-mapservices.html#setting-up-an-api",
    "href": "docs/developer/tutorial-data-management/7-providing-mapservices.html#setting-up-an-api",
    "title": "Providing convenience APIs",
    "section": "Setting up an API",
    "text": "Setting up an API\nMapserver is server software which is able to expose datasets through various APIs. Examples of similar software are QGIS server, ArcGIS Server, Geoserver and pygeoapi.\nWe’ve selected mapserver for this training, because of its robustness and low resource consumption. Mapserver is configured using a configuration file: called the mapfile. The mapfile defines metadata for the dataset and how users interact with the dataset, mainly the colour scheme (legend) to draw a map of the dataset.\nVarious tools exist to write these configuration files, such as Mapserver studio, GeoStyler, QGIS Bridge, up to a visual studio plugin to edit mapfiles.\nThe GeoDataCrawler, introduced in a previous paragraph, also has an option to generate mapfiles. A big advantage of this approach is the integration with existing metadata. GeoDataCrawler will, during mapfile generation, use the existing metadata, but also update the metadata so it includes a link to the mapserver service endpoint. This step enables a typical workflow of:\n\nUsers find a dataset in a catalogue\nThen open the dataset via the linked service\n\nBut also vice versa; from a mapping application, access the metadata describing a dataset.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Providing convenience APIs"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/7-providing-mapservices.html#mapfile-creation-exercise",
    "href": "docs/developer/tutorial-data-management/7-providing-mapservices.html#mapfile-creation-exercise",
    "title": "Providing convenience APIs",
    "section": "Mapfile creation exercise",
    "text": "Mapfile creation exercise\n\nNavigate with shell to a folder with data files.\nVerify if mcf’s are available for the files, if not, create initial metadata with crawl-metadata --mode=init --dir=.\nAdd a index.yml file to the folder. This metadata is introduced in the mapfile to identify the service.\n\nmcf:\n   version: 1.0\nidentification:\n    title: My new mapservice\n    abstract: A map service for data about ...\ncontact:\n  pointOfContact:\n    organization: ISRIC\n    email: info@isric.org\n    url: https://www.isric.org\n\nSet some environment variables\n\n\nLinuxPowershell\n\n\nexport pgdc_md_url=\"https://kenya.lsc-hubs.org/collections/metadata:main/items/{0}\"\nexport pgdc_ms_url=\"http://localhost\"\nexport pgdc_webdav_url=\"https://example.com/data\"\n\n\n$pgdc_md_url=\"https://kenya.lsc-hubs.org/collections/metadata:main/items/{0}\"\n$pgdc_ms_url=\"http://localhost\"\n$pgdc_webdav_url=\"https://example.com/data\"\n\n\n\n\nGenerate the mapfile\n\n\nLocalDocker & LinuxDocker & Powershell\n\n\ncrawl-maps --dir=.\n\n\ndocker run -it --rm -v$(pwd):/tmp \\\n  org/metatraining crawl-maps --dir=/tmp \n\n\ndocker run -it --rm -v \"${PWD}:/tmp\" `\n  org/metatraining crawl-maps --dir=/tmp \n\n\n\n\nIndex.yml may include a “robot” property, to guide the crawler in how to process the folder. This section can be used to add specific crawling behaviour.\n\nmcf:\n    version: 1.0\nrobot:\n    skip-subfolders: True # indicates the crawler not to proceed in subfolders\nYou can test this mapfile locally if you have mapserver installed. On windows, consider using conda or ms4w.\nconda install -c conda-forge mapserver\nMapserver includes a map2img utility, which enables to render a map image from any mapfile.\nmap2img -m=./mymap.map -o=test.png",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Providing convenience APIs"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/7-providing-mapservices.html#setup-mapserver-via-docker",
    "href": "docs/developer/tutorial-data-management/7-providing-mapservices.html#setup-mapserver-via-docker",
    "title": "Providing convenience APIs",
    "section": "Setup mapserver via Docker",
    "text": "Setup mapserver via Docker\nFor this exercise we’re using a mapserver image available from Docker hub.\ndocker pull camptocamp/mapserver:master  \nFirst create a config file, which we’ll mount as a volume into the container. On this config file we list all the mapfiles we aim to publish on our container. Download the default config file. Open the file and unescape and populate the maps section:\nMAPS\n     \"data\" \"/srv/data/data.map\"\nEND\nAlso unescape the OGCAPI templates section\nOGCAPI_HTML_TEMPLATE_DIRECTORY \"/usr/local/share/mapserver/ogcapi/templates/html-bootstrap4/\"\nIn the next statement we mount the data folder, including the config file and indicate on which port and with which config file the container will run:\n\nLinuxPowershell\n\n\ndocker run -p 80:80 \\\n    -e MAPSERVER_CONFIG_FILE=/srv/data/mapserver.conf \\\n    -v $(pwd):/srv/data  \\\n    camptocamp/mapserver:master \n\n\ndocker run -p 80:80 `\n    -e MAPSERVER_CONFIG_FILE=/srv/data/mapserver.conf `\n    -v \"${PWD}:/srv/data\" `\n    camptocamp/mapserver:master \n\n\n\nCheck http://localhost/data/ogcapi in your browser. If all has been set up fine it should show the OGCAPI homepage of the service. If not, check the container logs to evaluate any errors.\nYou can also try the url in QGIS. Add a WMS layer, of service http://localhost/data?request=GetCapabilities&service=WMS.\nNotice the links to metadata when you open GetCapabilities in a browser.\n\n\n\n\n\n\nNote\n\n\n\nIn recent years browsers have become more strict, to prevent abuse. For that reason it is important to carefully consider common connectivity aspects, when setting up a new service. Websites running at https can only embed content from other https services, so using https is relevant. CORS and CORB can limit access to embedded resources from remote servers. Using proper CORS headers and Content type identification, is relevant to prevent CORS and CORB errors.\n\n\nGeoDataCrawler uses default (gray) styling for vector and an average classification for grids. You can finetune the styling of layers through the robot section in index.yml.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Providing convenience APIs"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/7-providing-mapservices.html#summary",
    "href": "docs/developer/tutorial-data-management/7-providing-mapservices.html#summary",
    "title": "Providing convenience APIs",
    "section": "Summary",
    "text": "Summary\nIn this paragraph the standards of Open Geospatial Consortium have been introduced and how you can publish your data according to these standards using Mapserver. In the next section we’ll look at measuring service quality.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Providing convenience APIs"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/5-git-cicd.html",
    "href": "docs/developer/tutorial-data-management/5-git-cicd.html",
    "title": "GIT and CI/CD",
    "section": "",
    "text": "This page introduces a number of generic Git functionalities and vendor add ons. Which can support communities in efficient co-creation of content. The page mainly focusses on the Continuous Integration & Deployment functionality, but contains many external links to introduce other aspects of Git. Considering the previous materials, a relevant ci-cd case is a set of tasks to run after a change to some of the mcf documents in a data repository, to validate the mcf’s and convert them to iso19139 and push them to a catalogue.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "GIT and CI/CD"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/5-git-cicd.html#git-content-versioning",
    "href": "docs/developer/tutorial-data-management/5-git-cicd.html#git-content-versioning",
    "title": "GIT and CI/CD",
    "section": "GIT content versioning",
    "text": "GIT content versioning\nIn its core GIT is a version management system traditionally used for maintaining software codes. In case you never worked with GIT before, have a look at this Git & Github explanation. Some users interact with Git via the command line (shell). However excellent Graphical User Interfaces exist to work with Git repositories, such as Github Desktop, a Git client within Visual Studio, TortoiseGit, Smartgit, and many others.\nThese days GIT based coding communities like Github, Gitlab, Bitbucket offer various services on top of Git to facilitate in co-creation of digital assets. Those services include authentication, issue management, release management, forks, pull requests and CI/CD. The types of digital assets maintained via GIT vary from software, deployment scripts, configuration files, documents, website content, metadata records up to actual datasets. Git is most effective with text based formats, which explains the popularity of formats like CSV, YAML, Markdown.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "GIT and CI/CD"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/5-git-cicd.html#cicd",
    "href": "docs/developer/tutorial-data-management/5-git-cicd.html#cicd",
    "title": "GIT and CI/CD",
    "section": "CI/CD",
    "text": "CI/CD\nContinuous Integration & Deployment describes a process in which changes in software or configuration are automatically tested and deployed to a relevant environment. These processes are commonly facilitated by GIT environments. With every commit to the Git repository an action is triggered which runs some tasks.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "GIT and CI/CD"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/5-git-cicd.html#github-pages-exercise",
    "href": "docs/developer/tutorial-data-management/5-git-cicd.html#github-pages-exercise",
    "title": "GIT and CI/CD",
    "section": "Github Pages exercise",
    "text": "Github Pages exercise\nThis exercise introduces the CI-CD topic by setting up a basic markdown website in Github Pages, maintained through Git. Markdown is a popular format to store text with annotations on Git.The site will be based on Quarto. Quarto is one of many platforms to generate a website from a markdown repository.\n\nCreate a new repository in your github account, for example ‘My first CMS’. Tick the ’’\nBefore we add any content create a branch ‘gh-pages’ on the repository, this branch will later contain the generated html sources of the website.\nCreate file docs/index.md and docs/about.md. Start each file with a header:\n\n---\ntitle: Hello World\nauthor: Peter pan\ndate: 2023-11-11\n---\nAdd some markdown content to each page (under the header), for example:\n# Welcome\n\nWelcome to *my website*.\n\n- I hope you enjoy it.\n- Visit also my [about](./about.md) page.\n\nNow click on Actions in the github menu. Notice that Github has already set up a workflow to publish our content using jekyll, it should already be available at https://user.github.io/repo.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "GIT and CI/CD"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/5-git-cicd.html#using-quarto",
    "href": "docs/developer/tutorial-data-management/5-git-cicd.html#using-quarto",
    "title": "GIT and CI/CD",
    "section": "Using Quarto",
    "text": "Using Quarto\nIn LSC-hubs we’ve selected an alternative to jekyll, called quarto. In order to activate Quarto you need to set a number of items yourself.\n\nCreate a file _quarto.yml into the new git repository, with this content:\n\nproject:\n  type: website\nwebsite:\n  title: \"hello world\"\n  navbar:\n    left:\n      - href: index.md\n        text: Home\n      - about.md\nformat:\n  html:\n    theme: cosmo\n    toc: true\n\nRemove the existing workflow, generated by Github in Actions, Workflows, Remove\nFirst you need to allow the workflow-runner to make changes on the repository. For this, open Settings, Actions, General. Scroll down to Workflow permissions. Tick the Read and write permissions and click Save. If the option is grayed out, you first need to allow this feature in your organization.\nThen, from Actions, select New workflow, then set up a workflow yourself.\nOn the next page we will create a new workflow script, which is stored in the repository at /.github/workflows/main.yml.\n\nname: Docs Deploy\n\non:\n  push:\n    branches: \n      - main\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with: \n          tinytex: true \n          path: docs\n      - name: Publish to GitHub Pages (and render)\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n          path: docs\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      \n\nSave the file, via actions you can follow the progress of the workflow at every push to the repository.\nOn the logs notice how a container is initialised, the source code is checked out, the quarto dependency is installed, the build is made and pushed to the gh-pages branch.\n\nNotice that the syntax to define workflows is different for every CI-CD platform, however they generally follow a similar pattern. For Github identify in the file above:\n\nIt defines at what events the workflow should trigger (in this case at push events).\na build job is triggered, which indicates a container image (runs-on) to run the job in, then triggers some steps.\nThe final step triggers a facility of quarto to publish its output to a github repository\n\nThe above setup is optimal for co-creating a documentation repository for your community. Users can visit the source code via the edit on github link and suggest improvements via issues of pull requests. Notice that this tutorial is also maintained as markdown in Git.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "GIT and CI/CD"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/5-git-cicd.html#update-catalogue-from-git-ci-cd",
    "href": "docs/developer/tutorial-data-management/5-git-cicd.html#update-catalogue-from-git-ci-cd",
    "title": "GIT and CI/CD",
    "section": "Update catalogue from GIT CI-CD",
    "text": "Update catalogue from GIT CI-CD\nFor this scenario we need a database in the cloud to host our records (which is reachable by github workflows). For the training we suggest to use a trial account at elephantsql.com.\n\nAt elephantsql, create a new account.\nThen create a new Instance of type Tiny (free).\nClick on the instance and notice the relevant connection string (URL) and password\nConnect your instance of pycsw to this database instance, by updating pycsw.cfg and following the instructions at Catalogue publication\nVerify in elephantsql dashboard if the records are correctly loaded.\n\nWe will now publish our records from Github to our database.\n\nCreate a new repository on Github for the records\nMake sure git-scm (or a GUI tool like Git kraken, Smartgit) is intalled on your system.\nClone (download) the repository to a local folder.\n\ngit clone https://github.com/username/records-repo.git\n\nCopy the mcf files, which have been generated in Catalogue publication, to a datasets folder in the cloned repository.\nCommit and the files\n\ngit add -A && git commit -m \"Your Message\"\nBefore you can push your changes to Github, you need to set up authentication, generally 2 options are possible: - Using a personal access token - Or using SSH public key\ngit push origin main\nWe’ll now set up CI-CD to publish the records\n\nPlace the pycsw.cfg file in the root of the repository (including the postgres database connection)\nCreate a new custom workflow file with this content:\n\nname: Records Deploy\n\non: \n  push:\n    paths:\n      - '**'\n\ndefaults:\n  run:\n    working-directory: .\n\njobs:\n  build:\n    name: Build and Deploy Records\n    runs-on: ubuntu-latest\n    steps:\n        - uses: actions/checkout@v3\n        - uses: actions/setup-python@v4\n          with:\n              python-version: 3.9\n        - name: Install dependencies\n          run: |\n            sudo add-apt-repository ppa:ubuntugis/ppa\n            sudo apt-get update\n            sudo apt-get install gdal-bin\n            sudo apt-get install libgdal-dev\n            ogrinfo --version\n            pip install GDAL==3.4.3\n            pip install geodatacrawler pycsw sqlalchemy\n        - name: Crawl metadata\n          run: |\n            export pgdc_webdav_url=http://localhost/collections/metadata:main/items\n            export pgdc_canonical_url=https://github.com/pvgenuchten/data-training/tree/main/datasets/\n            crawl-metadata --dir=./datasets --mode=export --dir-out=/tmp\n        - name: Publish records\n          run: |   \n            pycsw-admin.py delete-records --config=./pycsw.cfg -y\n            pycsw-admin.py load-records --config=./pycsw.cfg  --path=/tmp\n\nVerify that the records are loaded on pycsw (through postgres)\nChange or add some records to GIT, and verify if the changes are published (may take some time)\n\nNormally, we would not add a connection string to a database in a config file posted on Github. Instead Github offers secrets to capture this type of information.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "GIT and CI/CD"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/5-git-cicd.html#cross-linking-catalogue-and-git",
    "href": "docs/developer/tutorial-data-management/5-git-cicd.html#cross-linking-catalogue-and-git",
    "title": "GIT and CI/CD",
    "section": "Cross linking catalogue and GIT",
    "text": "Cross linking catalogue and GIT\nWhile users are browsing the catalogue (or this page), they may find irregularities in the content. They can flag this as an issue in the relevant Git repository. A nice feature is to add a link in the catalogue page which brings them back to the relevant mcf in the git repository. With proper authorisations they can instantly improve the record, or suggest an improvement via an issue or pull request.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "GIT and CI/CD"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/5-git-cicd.html#summary",
    "href": "docs/developer/tutorial-data-management/5-git-cicd.html#summary",
    "title": "GIT and CI/CD",
    "section": "Summary",
    "text": "Summary\nIn this section you learned about using actions in Github (CI/CD). In the next section we are diving into data publication. Notice that you can also use GIT CI/CD mechanisms to deploy or evaluate metadata and data services.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "GIT and CI/CD"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/3-catalog-publication.html",
    "href": "docs/developer/tutorial-data-management/3-catalog-publication.html",
    "title": "Catalogue publication",
    "section": "",
    "text": "Catalogues facilitate data discovery in 3 ways:",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Catalogue publication"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/3-catalog-publication.html#catalogue-frontend",
    "href": "docs/developer/tutorial-data-management/3-catalog-publication.html#catalogue-frontend",
    "title": "Catalogue publication",
    "section": "Catalogue frontend",
    "text": "Catalogue frontend\nVarious catalogue frontends exist to facilitate dataset search, such as geonetwork, dataverse, ckan. Selecting a frontend depends on metadata format, target audience, types of data, maintenance aspects, and personal preference.\nFor this workshop we are going to use pycsw. It is a catalogue software supporting various standardised query APIs, as well as providing a basic easy-to-adjust html web interface.\nFor this exercise we assume you have docker-desktop installed on your system and running.\npycsw is available as docker image at docker hub, including an embedded SQLite database. In a production situation you will instead use a dedicated Postgres or MariaDB database for record storage.\n\nNavigate your shell to the temporary folder containing iso-xml documents. This folder will be mounted into the container, in order to load the records to the pycsw database.\n\n\nLinuxPowershell\n\n\ndocker run -p 8000:8000 \\\n   -v $(pwd):/etc/data \\\n   geopython/pycsw\n\n\ndocker run -p 8000:8000 `\n   -v \"${PWD}:/etc/data\" `\n   geopython/pycsw\n\n\n\n\nVisit http://localhost:8000\nMuch of the configuration of pycsw (title, contact details, database connection, url) is managed in a config file. Download the file to the current folder, adjust the title and restart docker with:\n\n\nLinuxPowershell\n\n\ndocker run -p 8000:8000 \\\n   -d --rm --name=pycsw \\\n   -v $(pwd):/etc/data \\\n   -v $(pwd)/pycsw.cfg:/etc/pycsw/pycsw.cfg \\\n   geopython/pycsw\n\n\ndocker run -p 8000:8000 `\n   -d --rm --name=pycsw `\n   -v \"${PWD}:/etc/data\" `\n   -v \"${PWD}/pycsw.cfg:/etc/pycsw/pycsw.cfg\" `\n   geopython/pycsw\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice -d starts the docker in the background, so we can interact with the running container. To see which instances are running (in the background) use docker ps. docker logs pycsw shows the logs of a container and docker stop pycsw stops the container. The -rm option removes the container at stop, so we can easily recreate it with additional options at next runs.\n\n\n\nFor administering the instance we use a utility called pycsw-admin.py. Notice on the calls below a reference to a relevant config file.\nFirst clear the existing database:\n\n\nContainer terminalPowershell\n\n\npycsw-admin.py delete-records -c /etc/pycsw/pycsw.cfg\n\n\ndocker exec -it pycsw bash -c \"pycsw-admin.py delete-records -c /etc/pycsw/pycsw.cfg\"\n\n\n\n\nNotice at http://localhost:8000/collections/metadata:main/items that all records are removed.\nLoad the records, which we exported as iso19139 in the previous section, to the database:\n\n\nContainer terminalPowershell\n\n\npycsw-admin.py load-records -p /etc/data/export -c /etc/pycsw/pycsw.cfg -y -r\n\n\ndocker exec -it pycsw bash -c `\n \"pycsw-admin.py load-records -p /etc/data/export -c /etc/pycsw/pycsw.cfg -y -r\"\n\n\n\n\nValidate at http://localhost:8000/collections/metadata:main/items if our records are loaded, else check logs to identify a problem.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Catalogue publication"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/3-catalog-publication.html#customise-the-catalogue-skin",
    "href": "docs/developer/tutorial-data-management/3-catalog-publication.html#customise-the-catalogue-skin",
    "title": "Catalogue publication",
    "section": "Customise the catalogue skin",
    "text": "Customise the catalogue skin\npycsw uses jinja templates to build the web frontend. These are html documents including template language to substitute parts of the page.\n\nSave the template below as a file ‘landing_page.html’ in the current directory\n\n{% extends \"_base.html\" %}\n{% block title %}{{ super() }} Home {% endblock %}\n{% block body %}\n&lt;h1&gt;Welcome to my catalogue!&lt;/h1&gt;\n&lt;p&gt;{{ config['metadata:main']['identification_abstract'] }}&lt;/p&gt;\nContinue to the records in this catalogue\n&lt;a title=\"Items\" \n    href=\"{{ config['server']['url'] }}/collections/metadata:main/items\"&gt;\n    Collections&lt;/a&gt;, or have a look at the  \n&lt;a title=\"OpenAPI\" \n      href=\"{{ config['server']['url'] }}/openapi?f=html\"&gt;Open API Document&lt;/a&gt;\n{% endblock %}\n\nWe will now replace the default template in the docker image with our template.\n\n\nLinuxPowershell\n\n\ndocker run -p 8000:8000 \\\n   -d --rm --name=pycsw \\\n   -v $(pwd):/etc/data \\\n   -v $(pwd)/pycsw.cfg:/etc/pycsw/pycsw.cfg \\\n   -v $(pwd)/landing_page.html:/etc/pycsw/ogc/api/templates/landing_page.html \\\n   geopython/pycsw\n\n\ndocker run -p 8000:8000 `\n   -d --rm --name=pycsw `\n   -v \"${PWD}:/etc/data\" `\n   -v \"${PWD}/pycsw.cfg:/etc/pycsw/pycsw.cfg\" `\n   -v \"${PWD}/landing_page.html:/home/pycsw/pycsw/pycsw/ogc/api/templates/landing_page.html\" `\n   geopython/pycsw\n\n\n\n\nView the result at http://localhost:8000\nHave a look at the other templates in pycsw\nWe published a tailored set of templates as a pycsw skin on github. This skin has been used as a starting point for the lsc-hubs catalogue skin.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Catalogue publication"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/3-catalog-publication.html#summary",
    "href": "docs/developer/tutorial-data-management/3-catalog-publication.html#summary",
    "title": "Catalogue publication",
    "section": "Summary",
    "text": "Summary\nIn this paragraph you learned how datasets can be published into a catalogue. In the next paragraph, we’ll look at importing metadata from external sources.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Catalogue publication"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/1-existing-resources.html",
    "href": "docs/developer/tutorial-data-management/1-existing-resources.html",
    "title": "Metadata at the source",
    "section": "",
    "text": "The FAIR principles are designed with the academic community in mind. Researchers sharing information. However the FAIR principles can also be applied within an organisation to improve data management, even on your personal computer. If you return for example to an archived project 2 years later to recover some data sources.\nMany organisations organise their documents and datasets on a central network storage or database. These resources are usually clustered in organisational units, projects and/or years. Some files and database tables in that central storage contain embedded metadata, such as the name, size, date, author, location etc. This information supports users in understanding the context of the data source. Especially if that data at some point is migrated from its original context.\nFor those formats which do not have embedded metadata, or in order to capture additional metadata aspects. We endorse the creation of a sidecar metadata file for every resource, a dedicated metadata file sharing the name of the datasource. This approach is for example common in the ESRI community, where a .shp.xml is created alongside any .shp file, which captures some metadata elements.\nThrough the embedded metadata and sidecar concept, we endorse data scientists to document their data at the source. Since they know best how the data is produced and how it should be used.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Metadata at the source"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/1-existing-resources.html#standardised-metadata-models",
    "href": "docs/developer/tutorial-data-management/1-existing-resources.html#standardised-metadata-models",
    "title": "Metadata at the source",
    "section": "Standardised metadata models",
    "text": "Standardised metadata models\nFor optimal interoperability, it is important to agree within your group on the metadata standard(s) to use in sidecar files. ESRI software for example provides an option to select the output model of the metadata. QGIS has various plugins, such as GeoCat Bridge, to work with various metadata models.\n\n\n\n\n\n\nTip\n\n\n\nDoes your organisation endorse a metadata model to describe data sources? Are your aware of tooling which can support you in creation of metadata in this model?",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Metadata at the source"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/1-existing-resources.html#the-mcf-format",
    "href": "docs/developer/tutorial-data-management/1-existing-resources.html#the-mcf-format",
    "title": "Metadata at the source",
    "section": "The MCF format",
    "text": "The MCF format\nWithin the geopython community a metadata format is used called the metadata control file (MCF). Aim of the format is ease of use, while providing export options to various metadata models. Many metadate models are based on XML, which makes them quite hard to read by humans. MCF is based on YAML, a text based format using indents to cluster elements. In this workshop we are using the MCF format due to its simplicity and natural fit with the use cases. A minimal sample of MCF is:\nmcf:\n    version: 1.0\n\nmetadata:\n    identifier: 9c36a048-4d28-453f-9373-94c90e101ebe\n    hierarchylevel: dataset\n    date: 2023-05-10\n\nidentification:\n    title: My favourite dataset\n    abstract: A sample dataset record to highlight the options of MCF\n    ...",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Metadata at the source"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/1-existing-resources.html#describing-a-resource",
    "href": "docs/developer/tutorial-data-management/1-existing-resources.html#describing-a-resource",
    "title": "Metadata at the source",
    "section": "Describing a resource",
    "text": "Describing a resource\nWhen describing a resource, consider which user groups are expected to read the information. This analyses will likely impact the style of writing in the metadata. The UK Geospatial Commission has published some practical recommendations on this topic.\nWhen tagging the dataset with keywords, preferably use keywords from controlled vocabularies like Agrovoc, Wikipedia, etc. Benefit of controlled vocabularies is that the term is not ambigue and it can be made available in multiple languages.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Metadata at the source"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/1-existing-resources.html#mcf-editing",
    "href": "docs/developer/tutorial-data-management/1-existing-resources.html#mcf-editing",
    "title": "Metadata at the source",
    "section": "MCF editing",
    "text": "MCF editing\nMCF documents can best be written in a text editor like Visual Studio Code. Consider to install the YAML plugin for instant YAML validation.\nAnother option to create and update mcf files is via MDME. MDME is a web based software package providing a dynamic metadata edit form. An operational package is available at osgeo.github.io. Notice that if you install the package locally, you can customize the application to your organisational needs.\n\n\n\n\n\n\nTip\n\n\n\nImagine a dataset you have recently worked with. Then open mdme and populate the form, describing that dataset. Now save the MCF file so we can later place it in a sample data repository.\nNotice that MDME also offers capabilities to export directly as iso19139, it uses a webservice based on the tools used in this workshop.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Metadata at the source"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/1-existing-resources.html#summary",
    "href": "docs/developer/tutorial-data-management/1-existing-resources.html#summary",
    "title": "Metadata at the source",
    "section": "Summary",
    "text": "Summary\nIn this section, you are introduced to a data management approach which maintains metadata at the location where the datasets are maintained, using a minimal, standards complient approach. You are introduced to the MCF metadata format. In the next section, we will go into more detail on interacting with the MCF format.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Metadata at the source"
    ]
  },
  {
    "objectID": "docs/developer/terria.html",
    "href": "docs/developer/terria.html",
    "title": "Terria JS",
    "section": "",
    "text": "TerriaJS is a web based map vizualisation tool. It can connect to a variety of file formats and online resources. You can share maps and map stories with partners and compare map views. For time enabled services you can create diagrams.",
    "crumbs": [
      "Developer",
      "Hub components",
      "Terria JS"
    ]
  },
  {
    "objectID": "docs/developer/terria.html#terriajs-in-lsc-hubs",
    "href": "docs/developer/terria.html#terriajs-in-lsc-hubs",
    "title": "Terria JS",
    "section": "TerriaJS in LSC-hubs",
    "text": "TerriaJS in LSC-hubs\nThis document describes how terriajs has been customised and deployed to fit the needs of LSC Hubs.\nTerriaMap is a ready-made web environment for the the terriaJS library.\nA docker container definition for TerriaMap is available at github.\nOn the container a number of files need to be overridden with tailored changes (to update titles, logo’s colors).\n\nwwwroot/about.html contains the text of the about page\nwwwroot/index.html adds a relevant title and some css updates\nwwwroot/config.json contains the config of the main application\ninit/simple.json and other map files in this folder can be used as predefined maps\n\nYou can either override these files as part of the container build process or add the files during container deployment.",
    "crumbs": [
      "Developer",
      "Hub components",
      "Terria JS"
    ]
  },
  {
    "objectID": "docs/developer/pycsw.html",
    "href": "docs/developer/pycsw.html",
    "title": "pycsw",
    "section": "",
    "text": "pycsw is a catalogue implementation in python, supporting a range of record exchange standards (CSW, oai-pmh, STAC, Opensearch) for optimal interoperability. Backend is PostGres, MariaDB or SQLite.",
    "crumbs": [
      "Developer",
      "Hub components",
      "pycsw"
    ]
  },
  {
    "objectID": "docs/developer/pycsw.html#pycsw-in-lsc-hubs",
    "href": "docs/developer/pycsw.html#pycsw-in-lsc-hubs",
    "title": "pycsw",
    "section": "pycsw in LSC-Hubs",
    "text": "pycsw in LSC-Hubs\npycsw has been set up with a PostGres backend. Every time records are updated on the git repository a CI/CD action updates the records in the database. The skin is based on a tailored skin, available on github.",
    "crumbs": [
      "Developer",
      "Hub components",
      "pycsw"
    ]
  },
  {
    "objectID": "docs/developer/index.html",
    "href": "docs/developer/index.html",
    "title": "Developer documentation",
    "section": "",
    "text": "Content management with Quarto\npycsw catalogue\nmapserver\nterriajs",
    "crumbs": [
      "Developer",
      "Hub components"
    ]
  },
  {
    "objectID": "docs/developer/index.html#tutorials",
    "href": "docs/developer/index.html#tutorials",
    "title": "Developer documentation",
    "section": "Tutorials",
    "text": "Tutorials\n\nTutorial data management",
    "crumbs": [
      "Developer",
      "Hub components"
    ]
  },
  {
    "objectID": "cases/swc.html",
    "href": "cases/swc.html",
    "title": "Kenya Soil Water Conservation",
    "section": "",
    "text": "Soil erosion is a major threat to sustainability and productivity with knock-on effects on the climate crisis and food security. It decreases soil fertility negatively affecting crop yields on fields, while eroded sediments from upstream clog downstream, dams, rivers and canals causing sedimentation and flooding."
  },
  {
    "objectID": "cases/swc.html#problem",
    "href": "cases/swc.html#problem",
    "title": "Kenya Soil Water Conservation",
    "section": "Problem",
    "text": "Problem\nSoil erosion is a major threat to sustainability and productivity with knock-on effects on the climate crisis and food security. It decreases soil fertility negatively affecting crop yields on fields, while eroded sediments from upstream clog downstream, dams, rivers and canals causing sedimentation and flooding. This is particularly true for hilly areas in Ethiopia, Kenya and Rwanda with high erosion risk. Soil erosion can be prevented and arrested through soil and water conservation (SWC) or sustainable land management (SLM) practices. Current land use and land management practices including crop cultivation as well as grazing do not sufficiently consider local potential, limitations and climate and soil erosion risks. While general information on improved land management may exist, context-based information on land resources, and climate and information on feasible SLM alternatives is not widely available or applied by land managers. ​ ​ ::: {.special-section}"
  },
  {
    "objectID": "cases/swc.html#goal",
    "href": "cases/swc.html#goal",
    "title": "Kenya Soil Water Conservation",
    "section": "Goal",
    "text": "Goal\nStakeholders informed of status of soil erosion risks and motivated to apply and promote more sustainable land use and land management practices through improved policies, plans and agricultural extension services, including contributions to Land Degradation Neutrality (LDN) monitoring and reporting.​ ​"
  },
  {
    "objectID": "cases/swc.html#scope",
    "href": "cases/swc.html#scope",
    "title": "Kenya Soil Water Conservation",
    "section": "Scope",
    "text": "Scope\nCatchment managers, authorities and farmer organisations can use the system to obtain information on the suitability of various sustainable land uses and land management practices for agricultural production and land restoration. These is based on existing data (soil, terrain, crop, climate), apps (decision support tools, etc.) and GIS based land suitability and soil erosion models, accessible in the LSC-hub.​"
  },
  {
    "objectID": "cases/swc.html#system",
    "href": "cases/swc.html#system",
    "title": "Kenya Soil Water Conservation",
    "section": "System",
    "text": "System\nLSC-IS hub, land suitability applications, Soil erosion and AEZ models and data used in national LDN monitoring and plans for County development, catchment management and farm management.​"
  },
  {
    "objectID": "cases/swc.html#scale",
    "href": "cases/swc.html#scale",
    "title": "Kenya Soil Water Conservation",
    "section": "Scale:",
    "text": "Scale:\n\nNational\nsub-national, landscape or catchment\nlocal level​"
  },
  {
    "objectID": "cases/swc.html#primary-actors",
    "href": "cases/swc.html#primary-actors",
    "title": "Kenya Soil Water Conservation",
    "section": "Primary actors",
    "text": "Primary actors\nNational: national and regional agriculture and forestry/natural resource management authorities, planners and project managers, UNCCD focal points.​\nLocal: catchment management, agricultural extension staff, farmer’s and other community-based organisations and authorities.​"
  },
  {
    "objectID": "cases/swc.html#secondary-actors",
    "href": "cases/swc.html#secondary-actors",
    "title": "Kenya Soil Water Conservation",
    "section": "Secondary actors",
    "text": "Secondary actors\nHub-hosts and administrator: EIAR, KALRO, RAB; related data providers.​"
  },
  {
    "objectID": "cases/swc.html#beneficiary",
    "href": "cases/swc.html#beneficiary",
    "title": "Kenya Soil Water Conservation",
    "section": "Beneficiary",
    "text": "Beneficiary\nFarmers and communities.​"
  },
  {
    "objectID": "cases/swc.html#scenario",
    "href": "cases/swc.html#scenario",
    "title": "Kenya Soil Water Conservation",
    "section": "Scenario​",
    "text": "Scenario​\nThe intermediary actor accesses the LSC-Hub’s web-based data system or portal, and finds existing information on Land, Soil and crops. The intermediary actor converts this information into land use and land management recommendations and advise to ultimate end users, in particular farmers.​"
  },
  {
    "objectID": "cases/models.html",
    "href": "cases/models.html",
    "title": "models",
    "section": "",
    "text": "Distribution of environmental phenomena in time and space as well as future yields at various scenario’s can be obtained using predictive modelling.\nFind models in the LSC catalogue"
  },
  {
    "objectID": "cases/isfm.html",
    "href": "cases/isfm.html",
    "title": "Kenya Integrated Soil Fertility Management",
    "section": "",
    "text": "Current fertilizer and soil fertility recommendations are not linked to local context, conditions and climate risks. Too general information may lead to declining soil fertility and soil health, low and uncertain productivity, fertilizer wastage, loss of organic matter, and environmental risks."
  },
  {
    "objectID": "cases/isfm.html#problem",
    "href": "cases/isfm.html#problem",
    "title": "Kenya Integrated Soil Fertility Management",
    "section": "Problem",
    "text": "Problem\nCurrent fertilizer and soil fertility recommendations are not linked to local context, conditions and climate risks. Too general information may lead to declining soil fertility and soil health, low and uncertain productivity, fertilizer wastage, loss of organic matter, and environmental risks. Integrated Soil Fertility Management (ISFM) has the potential to improve effectivity and efficiency of agronomic practices including both fertilizer recommendations and organic matter management and thereby boost crop production and farm income. Also, ISFM has the potential to deliver multiple co-benefits including climate adaptation (through improved water holding capacity and soil cover) as well as climate mitigation (through carbon sequestration).​"
  },
  {
    "objectID": "cases/isfm.html#goal",
    "href": "cases/isfm.html#goal",
    "title": "Kenya Integrated Soil Fertility Management",
    "section": "Goal",
    "text": "Goal\nImproved agronomic management advisories including crop type and soil fertility management to farmers via agricultural extension staff or directly.​"
  },
  {
    "objectID": "cases/isfm.html#scope",
    "href": "cases/isfm.html#scope",
    "title": "Kenya Integrated Soil Fertility Management",
    "section": "Scope",
    "text": "Scope\nLand users and their intermediaries can use the system to obtain information on the soil fertility and alternatives to improve soil fertility. It is based on existing data (soil, terrain, crop, climate), apps (decision support tools, etc.) and models (WOFOST, DSSAT etc) accessible in the LSC-hub. These will include risk assessment of weather and other external factors.​"
  },
  {
    "objectID": "cases/isfm.html#scale",
    "href": "cases/isfm.html#scale",
    "title": "Kenya Integrated Soil Fertility Management",
    "section": "Scale",
    "text": "Scale\n\nField/farm,\nSub-national (county/district/woreda)\nNational levels​"
  },
  {
    "objectID": "cases/isfm.html#primary-actors",
    "href": "cases/isfm.html#primary-actors",
    "title": "Kenya Integrated Soil Fertility Management",
    "section": "Primary actors",
    "text": "Primary actors\nnational: Extension staff, agricultural planner, agro-input provider\ndistrict: District extension officer, District Agric & Livestock officers and planners\nlocal: Local extension officer, local agro-input provider, lead farmer​"
  },
  {
    "objectID": "cases/isfm.html#secondary-actors",
    "href": "cases/isfm.html#secondary-actors",
    "title": "Kenya Integrated Soil Fertility Management",
    "section": "Secondary actors",
    "text": "Secondary actors\nHub-hosts and administrator: EIAR, KALRO, RAB; related data providers.​"
  },
  {
    "objectID": "cases/isfm.html#beneficiary",
    "href": "cases/isfm.html#beneficiary",
    "title": "Kenya Integrated Soil Fertility Management",
    "section": "Beneficiary",
    "text": "Beneficiary\nFarmers.​"
  },
  {
    "objectID": "cases/isfm.html#scenario",
    "href": "cases/isfm.html#scenario",
    "title": "Kenya Integrated Soil Fertility Management",
    "section": "Scenario ​",
    "text": "Scenario ​\nThe intermediary user accesses the LSC-Hub’s web-based data system or portal, and finds existing information on Land, Soil and crops. The intermediary actor – extension agent / lead farmer / agricultural input provider - converts this information into an agronomic and fertilizer recommendation and provides advise to end users, in particular farmers.​"
  },
  {
    "objectID": "cases/services.html",
    "href": "cases/services.html",
    "title": "Services",
    "section": "",
    "text": "Information services provide dedicated information derived from relevant data to a targeted audience.\nProcessing raw data in many cases requires expert knowledge. To bring the value of the data to a wider audience, it is of interest to set up information services derived from that data to targeted audiences.\nFind services in the LSC catalogue"
  },
  {
    "objectID": "docs/admin/index.html",
    "href": "docs/admin/index.html",
    "title": "Administration",
    "section": "",
    "text": "The current hub is published via Github. Github is a commercial service provider offering GIT services plus a number of add-ons to facilitate co-creation. Alternative GIT providers can be considered, such as gitlab, bitbucket. Notice that it is also possible to install gitlab on premise.\nAs an LSC hub administrator it is relevant to familiarise yourself with Github. Notice that some tools exist to facilitate the use of Git, such as Github desktop, Smartgit and Git Kraken. Some prefer the Github plugin of Visual Studio."
  },
  {
    "objectID": "docs/admin/index.html#github",
    "href": "docs/admin/index.html#github",
    "title": "Administration",
    "section": "",
    "text": "The current hub is published via Github. Github is a commercial service provider offering GIT services plus a number of add-ons to facilitate co-creation. Alternative GIT providers can be considered, such as gitlab, bitbucket. Notice that it is also possible to install gitlab on premise.\nAs an LSC hub administrator it is relevant to familiarise yourself with Github. Notice that some tools exist to facilitate the use of Git, such as Github desktop, Smartgit and Git Kraken. Some prefer the Github plugin of Visual Studio."
  },
  {
    "objectID": "docs/admin/index.html#lsc-hub",
    "href": "docs/admin/index.html#lsc-hub",
    "title": "Administration",
    "section": "LSC Hub",
    "text": "LSC Hub\nA number of content pages is made available to guide the user within the hub. These pages are maintained in github as markdown. Notice the edit link in the footer of each page. The pages are rendered to html using the quarto tool. Editing of quarto documents is facilitated by a quarto plugin in visual studio.\nThe quarto tool is configured as a ci-cd action in github. You can follow the progress of an action in the actions tab. In some cases an action may fail and human interaction is needed to fix the problem and/or restart the action."
  },
  {
    "objectID": "docs/admin/index.html#discussions",
    "href": "docs/admin/index.html#discussions",
    "title": "Administration",
    "section": "Discussions",
    "text": "Discussions\nDiscussions are managed within github. A giscus widget is added to every resource page, so users can provide feedback or ask questions about the resource. These questions are stored as github discussions. Discussions can be answered from the resource page (a github login is required) or from github discussions. Users should be invited to the github project to be able to administer github discussions."
  },
  {
    "objectID": "docs/admin/index.html#maps",
    "href": "docs/admin/index.html#maps",
    "title": "Administration",
    "section": "Maps",
    "text": "Maps\nThe TerriaJS framework enables users to share maps and map stories with stakeholders (share button top-right). Notice that you can also use this functionality to link from a description of a resource to a map, which displays some data in a certain context.\n[A nice map](https://maps.lsc-hubs.org/#start=%7B%22version%22%...)\nAnother option is to store the map definition (urldecoded) on a folder of TerriaJS. The map is then available by its filename. You can now add a link to this map for example in the related maps section of TerriaJS (config.json)."
  },
  {
    "objectID": "docs/admin/index.html#map-services",
    "href": "docs/admin/index.html#map-services",
    "title": "Administration",
    "section": "Map services",
    "text": "Map services\nMap services on source data are configured using mapserver mapfiles. The configuration is generated from the metadata of the source data using the geodatacrawler tool. Mapfiles are stored on a NFS (accessible via webdav) or in the mapserver (docker) container. Source of the mapfiles is preferably stored on Git.\nEvery new mapfile is registered in the mapserver config file as an alias"
  },
  {
    "objectID": "docs/admin/index.html#catalogue",
    "href": "docs/admin/index.html#catalogue",
    "title": "Administration",
    "section": "Catalogue",
    "text": "Catalogue\nThe catalogue can be linked to various information sources, without the data being stored on the LSC hub. Therefore, if you know existing information sources that are missing and should be added. Proving metadata is essential for findability and to avoid ambiguity.\nSource of catalogue records is maintained at github. Records are organised in Global, Continental, and country. Within country they are organised by portal/initiative.\nMetadata records are stored in the mcf format, a subset of iso19115 in a conveniant yaml encoding. But you can also add them as iso19115 xml format.\nA number of mechanisms is available to load records into the catalogue.\n\nImport records from external sources, such as data portals (zenodo, dataverse, CSW, STAC, OSF)\nMcf records can be created using mdme\nA Excel template is available, on which resources can be described. A single resource per record.\nAn ODK form is available on which users can describe resources\n\n\n\n\nODK"
  },
  {
    "objectID": "docs/developer/mapserver.html",
    "href": "docs/developer/mapserver.html",
    "title": "Mapserver",
    "section": "",
    "text": "MapServer is a platform for publishing spatial data to the web. Originally developed in the mid-1990’s at the University of Minnesota, MapServer is released under an MIT-style license, and runs on all major platforms (Windows, Linux, Mac OS X). MapServer is not a full-featured GIS system, nor does it aspire to be.",
    "crumbs": [
      "Developer",
      "Hub components",
      "Mapserver"
    ]
  },
  {
    "objectID": "docs/developer/mapserver.html#mapserver-in-lsc-hubs",
    "href": "docs/developer/mapserver.html#mapserver-in-lsc-hubs",
    "title": "Mapserver",
    "section": "Mapserver in LSC hubs",
    "text": "Mapserver in LSC hubs\nMapserver is used to create map services on data which is stored on the LSC hub. This makes the data better available, for example within TerriaJS.",
    "crumbs": [
      "Developer",
      "Hub components",
      "Mapserver"
    ]
  },
  {
    "objectID": "docs/developer/quarto.html",
    "href": "docs/developer/quarto.html",
    "title": "quarto",
    "section": "",
    "text": "Quarto is a content management framework. Content is maintained as markdown files. Python or R scripts can be included within markdown to create diagrams or maps. A visual studio plugin is available, which enables html previews within Visual Studio. You can also render the content to .docx, .pdf, .pptx and .revealjs.",
    "crumbs": [
      "Developer",
      "Hub components",
      "quarto"
    ]
  },
  {
    "objectID": "docs/developer/quarto.html#quarto-in-lsc-hubs",
    "href": "docs/developer/quarto.html#quarto-in-lsc-hubs",
    "title": "quarto",
    "section": "Quarto in LSC-hubs",
    "text": "Quarto in LSC-hubs\nThis documentation, as well as the hub website are generated with Quarto. The markdown files are stored on a git repository. CI/CD action has been set up, so with every push to the git repository, a new set of html pages is created and published to an online environment.",
    "crumbs": [
      "Developer",
      "Hub components",
      "quarto"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/0-fair-data.html",
    "href": "docs/developer/tutorial-data-management/0-fair-data.html",
    "title": "FAIR principles",
    "section": "",
    "text": "The FAIR principles are a good practice on data sharing in academic communities and beyond. A good starting point for our workshop on data management.\nIn this paragraph we present a number of exercises around the FAIR principles. And we’ll see how the principles work out in the soil domain specifically. FAIR Data is:",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "FAIR principles"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/0-fair-data.html#findable",
    "href": "docs/developer/tutorial-data-management/0-fair-data.html#findable",
    "title": "FAIR principles",
    "section": "Findable",
    "text": "Findable\nMetadata and data should be easy to find for both humans and computers.\n\nData and metadata have a unique persistent identifier\n\nA minimal approach to create a unique identifier (URI) is to combine a local identifier with a domain. For example: https://data.kalro.org/profiles/aa1-49bc-d11e\nIn theory, URIs do not need to resolve to an actual website, but it is a good practice to provide meaningfulcontent at each uri.\nDo not use product names and project names inURIs, it is difficult to maintain persistence.\nFrameworks such as DOI and ePIC offer a identification layer for online resources.\n\n\n\n\n\n\n\nTip\n\n\n\nFor the following datasets, review the uniqueness and persistence of their identifier and the identifier of the data.\n\n05b1e57a-8e31-4cdb-aca4-61ae3f21559d\n10.1016/j.rse.2019.111260\nd5bb6b02-0979-5112-8dd6-9aef6638fb73\nselect-nutrition-indicators-data-for-kenya-2022\n\nPerform the same analyses for some datasets on your local machine or organisation network.\n\n\n\n\nDescribe the data source with rich metadata\nRich metadata includes aspects such as, title, abstract, keywords, who is the author/owner of the resource, when was the resource created, are there any usage constraints, how does the resource relate to other resources.\n\n\n\n\n\n\nTip\n\n\n\nTo evaluate if a tiff-dataset containing texture-clay is relevant to answer your question on soil water availability, which aspects would you expect a metadata description of that dataset to include?\n\n\n\n\nMetadata are searchable in a catalogue\nIn order to find metadata efficiently, metadata records should be listed in a intuitive search interface\n\n\n\n\n\n\nTip\n\n\n\nNavigate to the following data portals and search for a dataset on for example soil texture in your area. Note down which aspects you would like to see improved to locate a dataset, or to know when to stop searching, because you assume you have located the best match in the catalogue.\n\nGeoportal.org\nGoogle dataset search\nData.isric.org\nDigitalearth.africa\nFAO catalogue",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "FAIR principles"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/0-fair-data.html#accessible",
    "href": "docs/developer/tutorial-data-management/0-fair-data.html#accessible",
    "title": "FAIR principles",
    "section": "Accessible",
    "text": "Accessible\nOnce a user finds the data, it should be clear how they can be accessed.\n\nMetadata and data are retrievable using a standardised communications protocol\nVarious communities adopted a range of standards for metadata exchange:\n\nMetadata\n\n\n\nCommunity\nStandard\nFormat/Protocol\n\n\n\n\nOpen data/Sematic web\nDCAT\nSPARQL\n\n\nScience\nDatacite\nOAI-PMH\n\n\nGeospatial\niso19115\nCSW/OGC API - Records\n\n\nEarth observation\nSTAC Catalog\nSTAC API\n\n\nSearch engines\nSchema.org\njson-ld/microdata\n\n\nEcology\nEML\nKNB/GBIF\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nA metadata model often is a combination of a schema and a format. Compare the following metadata records, identify which model is used in the record, what differences and communalities do you notice?\n\nAfrican open soil data\nWosis latest\nRainfall Chirps\nSoil excavation Assen\nKeileem Drenthe\n\n\n\n\n\nData\nMost common in data science is to provide a packaged version of a dataset and deploy it on a repository like Zenodo or Dataverse where it can be downloaded. Zenodo supports Findability and Accessibility of FAIR.\nIn the spatial and earth observation domain we tend to work with large files and the use of data APIs which allow to request subsets of the data are very common. The Open Geospatial Consortium has defined a number of standards for these APIs, so the APIs themselves are interoperable. The table below shows some of the common APIs. In the first column the older APIs, developed in the 90’s, in the second column their updated representative, recently adopted or still in development.\n\n\n\nService\nOGC API\nDescription\n\n\n\n\nWeb Map Service (WMS)\nMaps\nProvides a visualisation of a subset of the data\n\n\nWeb Feature Service (WFS)\nFeatures\nAPI to request a subset of the vector features\n\n\nWeb Coverage Service (WCS)\nCoverages\nAPI to interact with grid sources\n\n\nSensor Observation Service (SOS)\nSensorthings\nRetrieve subsets of sensor observations\n\n\n\nFrom the Earth Observation domain, an alternative mechanism is increasingly getting adopted. Complete files are stored on a public file repository, by creating an index on the file and enabling range requests, users are able to fetch subsets from the file directly (for which previously, you would have needed a WFS or WCS service).\nThis mechanism is enabled by new formats such as Cloud Optimised GeoTiff, GeoZarr, and GeoParquet.\n\n\n\nThe protocol allows for an authentication and authorisation procedure, where necessary\nFAIR endorses open access, however in some cases it is not possible to share some data to the global audience (privacy, economic, or safety concerns). It is still relevant to publish the data, so those authorised can access it. This requires a proper level of authorisation and authentication being set up.\n\n\nMetadata are accessible, even when the data are no longer available\nMetadata models usually have a status field, which enables you to indicate that a resource has been archived. The metadata would still be available, so users are aware it once existed.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "FAIR principles"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/0-fair-data.html#interoperable",
    "href": "docs/developer/tutorial-data-management/0-fair-data.html#interoperable",
    "title": "FAIR principles",
    "section": "Interoperable",
    "text": "Interoperable\nData typically are integrated with other data, as well as interoperate with applications or workflows for analysis, storage, and processing.\n\n(Meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.\nThe soil community has a long history of interoperability efforts for soil profile data. Such as:\n\nGlobalsoilmap\ne-Soter\niso28258:2013\n\n\ne-Soter\nThe e-Soter model has been developed in the e-Soter Research project, based on principles of previous SOil TERrain (SOTER) initiatives. e-Soter is a relational database model, usually implemented as a Microsoft Access database. Some examples of e-Soter implementations:\n\nMalawi\nKenya\nSouthern Africa\nSenegal and Gambia\n\n\n\niso28258:2013\nIn 2012 various experts in the soil domain grouped around the development of the first formally standardised domain model on soil data, published as ISO28258. ISO28258 adopted the Observations & Measurements conventions of OGC. Each observation on a site, profile, horizon or soil sample is considered an observation. For each observation on a specimen, the measured property and the procedure are captured.\n\n\n\nObservations and measurements overview\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor this exercise we are considering a dataset on observations on soil properties (either field or lab). If you are aware of such a dataset in your organisation or region, use that one, else you can use the KENSOTER dataset. Then answer the following questions:\n\nDescribe the dataset; by region, format, organisation, date, …\nIn which column/property are the observed result values stored?\nHow is the observation linked to the location and depth of the sample?\nWhere is documented the unit used for the value?\nHow is documented which soil property is measured\nWhere is described which procedure/method has been used for this observation?\nAre metadata about the measurement available, when was the sample analysed, who made the sample, which lab?\n\n\n\nVarious initiatives adopted ISO28258, and serialised and specialised the model for their community:\n\nINSPIRE Soil (Europe) and ANZSoilML (Australia) are domain models based on Observations and Measurements, inspired by ISO28258, serialised in GML.\nISO28258-relational is an implementation of ISO28258 modelled as a relational database.\nGlosis Web Ontology is an evolution of iso28258, using common ontologies from the web, such as semantic sensor network.\n\nSome examples of datasets modelled as INSPIRE Soil:\n\nSoil Berlin\nSoil chemistry Flanders\nSoil Poland\n\n\n\n\n\n\n\nTip\n\n\n\nDownload a Soil GML file and try to open it in QGIS. QGIS usually is able to display the profile locations. Alternatively you can use the GML Appschema format in OGR to generate first a SQLite database of the file, before opening it in QGIS.\n\n\n\n\n\n(Meta)data use vocabularies that follow FAIR principles\nA number of common vocabularies are relevant to the soil domain.\nThe World Reference Base for soil resources provides a framework of code lists on soil and soil classification. These lists are partially published in Agrovoc and partially in Glosis web ontology.\n\n\n\n\n\n\nTip\n\n\n\nExamine the concept Durisols in agrovoc.\n\nNotice that the agrovoc page on Durisols looks nicer then the representation linked to its uri. Still it is important to use the persistent identifier when linking to the concept, why?\nNotice that Agrovoc contains many translations for each concept and linkage to wider and narrower terms. These are some of the benefits of linking to a keyword from a common thesaurus.\n\n\n\n\n\nMetadata include qualified references to other metadata\nThe context of a dataset gets more clear if you link it to datasets which were used as a source, documents in which it is described, tools with which it has been produced or which tool can be used to view/process it, policies for which it has been created, etc. Consider that users also may traverse the link, to find datasets relevant to a certain policy or tool.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "FAIR principles"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/0-fair-data.html#reusable",
    "href": "docs/developer/tutorial-data-management/0-fair-data.html#reusable",
    "title": "FAIR principles",
    "section": "Reusable",
    "text": "Reusable\nReuse of data is the main goal of FAIR, facilitated by documentation of the data, for different audiences.\n\nUse a clear and accessible data usage license\nUsers are very interested to know if and how they can use the data. This process is facilitated by adoption of a commonly available license, such as odbl or cc-by, so users (and machines) can identify the applicable license without reading a full document.\n\n\n\n\n\n\nTip\n\n\n\nDoes your organisation provide guidance on which license to use on various data sources? Is it clear when, and when not to use an open license? Are you aware of any data sources which currently do not yet have an assigned data license?\n\n\n\n\nData are associated with detailed provenance\nProvenance is the process of creation and curation of a data source. Which data sources or procedures were used to create the data source. Which processing steps have been applied to the data. What is the lifecycle of the dataset (when will it be archived).\nThis information is very relevant to potential users of the data, because they can understand if the data has been produced according to their expectations.\nIn academia provenance and processing are usually described in scientific articles. One can also capture these aspects in a linked metadata record. Some tools (for example ArcGIS and SPSS) create a processing log automatically.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "FAIR principles"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/0-fair-data.html#summary",
    "href": "docs/developer/tutorial-data-management/0-fair-data.html#summary",
    "title": "FAIR principles",
    "section": "Summary",
    "text": "Summary\nIn this section you learned about the FAIR principles and how this applies to the soil data community. In the next sections we will introduce a data management strategy we use on some of our projects. We expect some of the presented tools may be worthwile to have a closer look at, to see if it can support you in your daily tasks.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "FAIR principles"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/2-interact-with-data-repositories.html",
    "href": "docs/developer/tutorial-data-management/2-interact-with-data-repositories.html",
    "title": "Interact with data repositories",
    "section": "",
    "text": "In this section a crawler tool is introduced which let’s you interact with the metadata in a file based data repository. For this exercise we’ve prepared a minimal data repository containing a number of Excel-, Shape- and Tiff-files. Unzip the repository to a location on disk.\nIn the root folder of the repository already exists a minimal mcf file, index.yml. This file contains some generic metadata properties which are used if a file within the repository does not include them. The tool we use is able to inherit metadata properties from this index.yml file through the file hierarchy.\nConsider to add additional index.yml files in other folders to override the values of index.yml at top level.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Interact with data repositories"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/2-interact-with-data-repositories.html#setup-environment",
    "href": "docs/developer/tutorial-data-management/2-interact-with-data-repositories.html#setup-environment",
    "title": "Interact with data repositories",
    "section": "Setup environment",
    "text": "Setup environment\nThe tool we will use is based on python. It has some specific dependencies which are best installed via Conda. Conda creates a virtual python environment, so any activity will not interfere with the base python environment of your machine. Notice that the next paragraph describes an alternative approach without installing python locally.\nIf you don’t have Conda, you can install Anaconda or Miniconda and consider to read the getting started.\nNow start a commandline or powershell with conda enabled (or add conda to your PATH). On windows look for the Anaconda prompt in start menu. First we will navigate to the folder in which we unzipped the sample data repository. Make sure you are not in the data directory but one above.\ncd {path-where-you-unzipped-zipfile}\nWe will create a virtual environment pgdc (using Python 3.9) for our project and activate it.\nconda create --name pgdc python=3.11 \nconda activate pgdc\nNotice that you can deactivate this environment with: conda deactivate and you will return to the main Python environment. The tools we will install below, will not be available in the main environment.\nInstall the dependencies for the tool:\nconda install -c conda-forge gdal\nconda install -c conda-forge pysqlite3\nconda install -c conda-forge pandas\nNow we will install the crawler tool, GeoDataCrawler. The tool is under active development at ISRIC and facilitates many of our data workflows. It is powered by some popular metadata and transformation libraries; OWSLib, pygeometa and GDAL.\npip install geodatacrawler\nVerify the different crawling options by typing:\ncrawl-metadata --help",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Interact with data repositories"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/2-interact-with-data-repositories.html#pythongdal-via-docker",
    "href": "docs/developer/tutorial-data-management/2-interact-with-data-repositories.html#pythongdal-via-docker",
    "title": "Interact with data repositories",
    "section": "Python/GDAL via Docker",
    "text": "Python/GDAL via Docker\nIn case you have difficulties setting up python with gdal on your local machine (or just want to try out), an alternative approach is available, using python via Docker. Docker is a virtualisation technology which runs isolated containers within your computer.\n\nFirst install docker.\nThen in a blank folder create 2 files: Dockerfile\n\nFROM mambaorg/micromamba:1.5.3\nCOPY --chown=$MAMBA_USER:$MAMBA_USER env.yaml /tmp/env.yaml\nRUN micromamba install --yes --file /tmp/env.yaml\nRUN micromamba install -y -n base -c conda-forge gdal pandas pysqlite3\nRUN micromamba clean --all --yes\nARG MAMBA_DOCKERFILE_ACTIVATE=1\nRUN pip install geodatacrawler\n\nand env.yaml\n\nname: base\nchannels:\n  - conda-forge\ndependencies:\n  - pyopenssl=20.0.1\n  - python=3.9.1\n  - requests=2.25.1\n\nBuild the geodatacrawler image (an image is a blueprint to create containers). Navigate with powershell or commandline to the folder you have just created.\n\ndocker build -t org/metatraining .\n\nNow navigate to the folder where you unzipped the data repository and use the docker image to run the crawler:\n\ndocker run -it --rm org/metatraining crawl-metadata --help\nFor advanced docker statements there are some differences between Windows commandline, Windows powershell and Linux. Use the relevant syntax for your system.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Interact with data repositories"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/2-interact-with-data-repositories.html#initial-mcf-files",
    "href": "docs/developer/tutorial-data-management/2-interact-with-data-repositories.html#initial-mcf-files",
    "title": "Interact with data repositories",
    "section": "Initial MCF files",
    "text": "Initial MCF files\nThe initial task for the tool is to create for every data file in our repository a sidecar file based on embedded metadata from the resource.\n\nLocalDckr & LinuxDckr & Powershell\n\n\ncrawl-metadata --mode=init --dir=data\n\n\ndocker run -it --rm -v$(pwd):/tmp \\\n org/metatraining crawl-metadata \\\n --mode=init --dir=tmp/data\n\n\ndocker run -it --rm -v \"${PWD}:/tmp\" `\n  org/metatraining crawl-metadata `\n  --mode=init --dir=/tmp/data\n\n\n\nNotice that for each resource a {dataset}.yml file has been created. Open a .yml file in a text editor and review the content.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Interact with data repositories"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/2-interact-with-data-repositories.html#update-mcf",
    "href": "docs/developer/tutorial-data-management/2-interact-with-data-repositories.html#update-mcf",
    "title": "Interact with data repositories",
    "section": "Update MCF",
    "text": "Update MCF\nThe update mode is meant to be run at intervals, it will update the mcf files if changes have been made on a resource.\n\nLocalDckr & LinuxDckr & Powershell\n\n\ncrawl-metadata --mode=update --dir=data\n\n\ndocker run -it --rm -v$(pwd):/tmp \\\n org/metatraining crawl-metadata \\\n --mode=update --dir=tmp/data\n\n\ndocker run -it --rm -v \"${PWD}:/tmp\" `\n  org/metatraining crawl-metadata `\n  --mode=update --dir=/tmp/data\n\n\n\nIn certain cases the update mode will also import metadata from remote url’s. This happens for example if the dataset-uri is a DOI. The update mode will ten fetch metadata of the DOI and push it into the MCF.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Interact with data repositories"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/2-interact-with-data-repositories.html#export-mcf",
    "href": "docs/developer/tutorial-data-management/2-interact-with-data-repositories.html#export-mcf",
    "title": "Interact with data repositories",
    "section": "Export MCF",
    "text": "Export MCF\nFinally we want to export the MCF’s to actual iso19139 metadata to be loaded into a catalogue like pycsw, GeoNetwork, CKAN etc.\n\nLocalDckr & LinuxDckr & Powershell\n\n\ncrawl-metadata --mode=export --dir=data --dir-out=export --dir-out-mode=flat\n\n\ndocker run -it --rm -v$(pwd):/tmp \\\n org/metatraining crawl-metadata \\\n --mode=export --dir=tmp/data \\\n --dir-out=export --dir-out-mode=flat\n\n\ndocker run -it --rm -v \"${PWD}:/tmp\" `\n  org/metatraining crawl-metadata `\n  --mode=export --dir=/tmp/data `\n  --dir-out=/tmp/export --dir-out-mode=flat\n\n\n\nOpen one of the xml files and evaluate if the contact information from step 1 is available.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Interact with data repositories"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/2-interact-with-data-repositories.html#summary",
    "href": "docs/developer/tutorial-data-management/2-interact-with-data-repositories.html#summary",
    "title": "Interact with data repositories",
    "section": "Summary",
    "text": "Summary\nIn this paragraph you have been introduced to the geodatacrawler library. In the next section we are looking at catalogue publication.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Interact with data repositories"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/4-bulk-import.html",
    "href": "docs/developer/tutorial-data-management/4-bulk-import.html",
    "title": "Bulk import",
    "section": "",
    "text": "This paragraph describes approaches to import metadata from existing repositories. Including an option to import metadata from records of a spreadsheet.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Bulk import"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/4-bulk-import.html#bulk-import-from-spreadsheet",
    "href": "docs/developer/tutorial-data-management/4-bulk-import.html#bulk-import-from-spreadsheet",
    "title": "Bulk import",
    "section": "Bulk import from spreadsheet",
    "text": "Bulk import from spreadsheet\nMany metadata initiatives tend to start from a spreadsheet. Each of the columns representa a metadata property and the rows are the individual records describing a resource. Spreadsheets have proven to be an effective medium to populate a catalogue with records initially. To facilitate this use case the GeoDataCrawler software provides an import spreadsheet method. The spreadsheet is parsed and a MCF document is generated for every row.\nSince every metadata initiative tends to have dedicated columns. A templating approach is used to convert from row to MCF. A default template is available, matching a default spreadsheet layout. If your spreadsheet layout is different, you need to adjust the template accordingly.\n\nFor this exercise we’ll use the LSC-hubs spreadsheet in combination with the LSC-hubs template. Notice that the template has the same filename, but with extension .j2. Download both files to a new folder, called csv, in your working directory.\nFrom your shell environment run this command:\n\n\nLocalDckr & LinuxDckr & Powershell\n\n\ncrawl-metadata --mode=import-csv --dir=\"./csv\"\n\n\ndocker run -it --rm -v$(pwd):/tmp \\\n  org/metatraining crawl-metadata \\\n  --mode=import-csv --dir=\"/tmp/csv\"\n\n\ndocker run -it --rm -v \"${PWD}:/tmp\" `\n  org/metatraining crawl-metadata `\n  --mode=import-csv --dir=\"/tmp/csv\"\n\n\n\n\nIf there are errors, check the paths and consider to open the CSV in Google Sheets and export it again or open it in a text editor to look for special cases. A known issue with this approach is that the crawler tool can not manage newline characters in text fields.\nOpen one of the generated MCF files to evaluate its content.\nA common spreadsheet tool is Microsoft Excel. If you open and export a spreadsheet from Excel, the CSV will use the ‘;’ character as column separator. Use the –sep=‘;’ parameter to indicate GeoDataCrawler to use this separator.\n\ncrawl-metadata --mode=import-csv --dir=\"./csv\" --sep=';'",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Bulk import"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/4-bulk-import.html#bulk-import-from-an-online-location",
    "href": "docs/developer/tutorial-data-management/4-bulk-import.html#bulk-import-from-an-online-location",
    "title": "Bulk import",
    "section": "Bulk import from an online location",
    "text": "Bulk import from an online location\nMany resources are already described elsewhere which may be of interest to add to our catalogue. For this use case some options exist to import remote metadata.\n\nBy ID\nIn many cases you want to import a selected subset of a remote catalogue. In this scenario we prepare a csv with the identifiers of the records to be imported. These identifiers are used to create a very minimal MCF file. GeoDataCrawler subsequently extends the local record with remote content. Currently supported are metadata from CSW, WMS and DOI.\n\nFor this exercise download a spreadsheet with a subset of the isric.org data repository and the relevant template to a folder isric.\nImport the CSV\n\ncrawl-metadata --mode=import-csv --dir=\"./isric\" --sep=';'\n\nThe synchronisation with remote content is implemented in GeoDataCrawler as part of the update metadata method. The update process will evaluate the dataseturi-property. If the uri refers to remote content in a supported format, the MCF will be updated against that content (remote takes preference).\n\ncrawl-metadata --mode=update --dir=\"./isric\" --resolve=true\n\nNotice the changes before and after running the script. If needed, you can remove all the MCF files and run import-csv again to restore the originals.\n\n\n\nHarvest full set\nIn case you want to harvest the full set of a remote catalogue, you can create a basic MCF in a new folder undrr and add a distribution of a CSW endpoint.\nmetadata:\n    hierarchylevel: service\n    identifier: riskprofilesundrr\ndistribution:\n  csw:\n    name: csw\n    url: http://riskprofilesundrr.org/catalogue/csw\n    type: OGC:CSW\nNow use the crawler to fetch the remote records of the catalogue.\ncrawl-metadata --mode=update --dir=\"./undrr\" --resolve=true\nYou can repeat this harvest at intervals to keep your catalogue up to date with the remote.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Bulk import"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/4-bulk-import.html#summary",
    "href": "docs/developer/tutorial-data-management/4-bulk-import.html#summary",
    "title": "Bulk import",
    "section": "Summary",
    "text": "Summary\nWe’ve seen a number of options to import metadata from external sources. In the next section we’ll have a look at Git, a versioning system.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Bulk import"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/6-data-publication.html",
    "href": "docs/developer/tutorial-data-management/6-data-publication.html",
    "title": "Data publication",
    "section": "",
    "text": "In order to share a dataset with colleagues, partners or the wider public. The file should be published in a shared environment. Various technologies are available to share a file on a network. To select a relevant location mainly depends on which type of users are going to access the dataset.\nThe following options exist:",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Data publication"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/6-data-publication.html#persistent-identification",
    "href": "docs/developer/tutorial-data-management/6-data-publication.html#persistent-identification",
    "title": "Data publication",
    "section": "Persistent identification",
    "text": "Persistent identification\nSelect the location carefully, prevent to regularly move a file to a new location. Because with every move, users need to be notified of the new location and documentation needs to be updated. When moving is relevant, consider to set up a forward-rule on the server which redirects users to the new location.\nAn interesting aspect of persistent identification is the choice of a domain and path name. A domain should represent enough credibility/authority (is this a trusted resource), and should be persistent for a longer period. A project name, for example, is not a good choice for a domain to publish a resource.\nA mechanism exists which facilitates file moves, without breaking their identification. Identifier providers such as DOI and ePIC enable creation of a identifier for any resource. In documentation use the provided identifier, in case the location of the resource changes, you can update the link behind the identifier. Some organisations install a identification service themselves, so they have full ownership on the domain and the contents of the service. An example of such a service is the identification service of the German Federal Government.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Data publication"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/6-data-publication.html#include-metadata",
    "href": "docs/developer/tutorial-data-management/6-data-publication.html#include-metadata",
    "title": "Data publication",
    "section": "Include metadata",
    "text": "Include metadata\nFor optimal discoverability, it is important to combine data publication with metadata. Either via embedded metadata in the file, else with a separate metadata file. In case of a shared folder or cloud service, embed or place the metadata along the data files, so people browsing through the system can easily find it.\nThe embedded or sidecar metadata should be duplicated into catalogue software, to make it searchable by the targeted audience. This process is further described at catalogue publication.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Data publication"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/6-data-publication.html#summary",
    "href": "docs/developer/tutorial-data-management/6-data-publication.html#summary",
    "title": "Data publication",
    "section": "Summary",
    "text": "Summary\nVarious technologies exist to share data on a network. When selecting a mechanism, evaluate if you can facilitate identifier persistence and share metadata along with the files. In the next section we’ll setup convenience APIs on data to facilitate reuse of the data.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Data publication"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/8-measure-quality.html",
    "href": "docs/developer/tutorial-data-management/8-measure-quality.html",
    "title": "Quality of Service",
    "section": "",
    "text": "Quality of service monitoring supports data providers to understand strengths and weaknesses of a system. Aspects which are monitored are:\nQuality of service monitoring is a standard activity in IT. Therefore consult your IT department or hosting company if they have tools available to assess these aspects. Confirm with them on how to extend and/or share with you the registrations for the requested parameters. Combine these reports into monthly or quarterly reports to facilitate system maintenance and roadmap.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Quality of Service"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/8-measure-quality.html#availability-monitoring",
    "href": "docs/developer/tutorial-data-management/8-measure-quality.html#availability-monitoring",
    "title": "Quality of Service",
    "section": "Availability Monitoring",
    "text": "Availability Monitoring\nTo assess the availability of a service, it requires to monitor the availability of the service at intervals. A basic availability-test every 5 minutes is usually sufficient. Many software exists for availability monitoring, such as Zabbix, Nagios, CheckMK, pingdom.\nIn this workshop we’re introducting the a Python based monitoring solution, called GeoHealthCheck. Geohealthcheck is designed to monitor spatial data services specifically.\nThe following exercise assumes docker desktop to be installed. Alternatively you can create a personal account at https://demo.geohealthcheck.org (click register in the login page).\n\n\n\n\n\n\nTip\n\n\n\n\nStart by setting up a local GeoHealthCheck container:\n\ndocker run -p80:80 geopython/geohealthcheck\n\nVisit http://localhost\nLogin as user: admin, password: admin\nClick ADD + on the top bar right, select WMS\nAdd a WMS url, for example https://maps.isric.org/mapserv?map=/map/wrb.map\nOn the next screen add WMS Drilldown (so all layers are validated)\nClick Save and test\nWhen finished, click Details to see the test result\n\n\n\nThis test is automatically repeated at intervals (as long the service is running). You can return to the test page to view a diagram of the availability over time. You can also configure a e-mail address to recieve notifications if a service is not available.\nRead more at the GeoHealthCheck website,\nAn advanced aspect of availability is identifying broken links on your content. Many tools exist which can monitor wensites on broken links at intervals, for example W3C linkcheck. Also google search console can be used to identify broken links. Another option is to verify in your website access logs if there are any requests which returned a 404 status, if such a request has a referer url, you are able to identify which website incorrectly linked to one of your resources.\n\n\n\n\n\n\nTip\n\n\n\nVisit https://validator.w3.org/checklink and fill in a website you maintain or frequently visit. The check button starts a process to evaluate broken links.\n\n\nSometimes a machine is not able to identify if a link is broken, for example if the target does not return a typical 404 Not found message.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Quality of Service"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/8-measure-quality.html#performance-capacity",
    "href": "docs/developer/tutorial-data-management/8-measure-quality.html#performance-capacity",
    "title": "Quality of Service",
    "section": "Performance & Capacity",
    "text": "Performance & Capacity\nTo know the capacity and performance of a service you can perform some load tests prior to moving to production. An alternative approach to evaluate performance is to extract the access logs of the service into an aggregation tool like AWStats or Kibana and evaluate the number of requests exceeding the limits.\n\n\n\n\n\n\nNote\n\n\n\nA common challenge to service performance in spatial data is the provision of a WMS service on a big dataset. When requesting that dataset on a continental or national level, the server runs into problems drawing all the data at once. In such case consider to set up some cache/aggregation mechanism for larger areas. Setting proper min/max scale denominators can be a solution also.\n\n\njmeter is a utility which can run a series of performance and capacity tests on a webservice. Jmeter is a java program, which can run on most platforms.\n\nDownload the latest version from the apache website.\nUnzip the archive and run jmeter.bat from bin directory.\nFollow the build web test plan tutorial.\nCustomise the web test plan for your mapserver service\n\n\n\n\n\n\n\nNote\n\n\n\nDo not perform a load test against a production url, it wil severely impact the performance of that service.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Quality of Service"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/8-measure-quality.html#usage-monitoring",
    "href": "docs/developer/tutorial-data-management/8-measure-quality.html#usage-monitoring",
    "title": "Quality of Service",
    "section": "Usage monitoring",
    "text": "Usage monitoring\nTo capture the usage of a service you can extract the usage logs and import them in a tool like Kibana, Splunk, Matomo or AW stats. For spatial data, it is interesting to define rules to extract the requested layer name from a WMS request.\nAWStats is a basic utility to report on service usage.\n\nNavigate to an empty folder, place a sample log file in the folder, rename the file to access.log.\nStart a container\n\n\nLinuxPowershell\n\n\ndocker run -d --restart always --publish 3000:80 \\\n --name awstats --volume $(pwd):/var/local/log:ro \\\n pabra/awstats\n\n\ndocker run -d --restart always --publish 3000:80 `\n --name awstats -v \"${PWD}:/var/local/log:ro\" `\n pabra/awstats\n\n\n\n\nParse the logs:\n\ndocker exec awstats awstats_updateall.pl now\n\nView the dashboard at http://localhost:3000, navigate to May 2015 to see the parsed logs from the sample.\n\n\n\nSearch Engine Optimisation\nAs part of usage monitoring it is also of interest to understand how users find your data via search engines. The popular search engines offer tooling to report on how the crawlers navigate through your data and how users find your services. You need to verify ownership of a domain either via a DNS property or by uploading an identification string to the website.\nYou can also use Google rich result test to extract structured data from a website. Or go to the dataset search engine to understand if and how the search engine finds your data.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Quality of Service"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/8-measure-quality.html#summary",
    "href": "docs/developer/tutorial-data-management/8-measure-quality.html#summary",
    "title": "Quality of Service",
    "section": "Summary",
    "text": "Summary\nIn this section you learned about various mechanisms to evaluate, report about and improve service quality. This concludes the main part of the workshop. In the last section we collected some advanced options in case you want to learn more details about data management.",
    "crumbs": [
      "Developer",
      "Workshop Data",
      "Quality of Service"
    ]
  },
  {
    "objectID": "docs/developer/tutorial-data-management/index.html",
    "href": "docs/developer/tutorial-data-management/index.html",
    "title": "Workshop on effective spatial data management",
    "section": "",
    "text": "This tutorial presents a workshop on effective (spatial) data maintenance. The tutorial is under active development, we welcome you to leave your feedback.\nThe first section introduces the FAIR data principles, focusing on the soil domain, which form a basis of the data management approach suggested in the next sections.\nOn the tutorial some tools are introduced, which are selected considering:\n\nOpen source license\nVivid community (persistence)\nSupport of standards (prevent vendor lock-in)\n\nIncidentally alternative options are indicated, because other tools may fit better with existing practices in your organisation.\nGit has a central role in the training, it is for example suggested to facilitate software development and deployment, content co-creation and management as well as community feedback.\nYou can also access the slides of the workshop.\nWe hope you enjoy the materials,\nThaïsa, Paul, Giulio and Luis.",
    "crumbs": [
      "Developer",
      "Workshop Data"
    ]
  },
  {
    "objectID": "docs/user/index.html",
    "href": "docs/user/index.html",
    "title": "User Guideline LSC Hubs",
    "section": "",
    "text": "This document is a guideline for usage of the Land Soil Crop hub.\n\n\nDecision support tools in agronomy heavily depend on acurate environmental and crop data. Many data are available, but often disperse and hard to locate. Land Soil Crop hubs aim to faciliate findability and accessibility of relevant data. Hubs facilitate data and knowledge to be stored, described, processed and visualized.\nThe hubs describe a number of different resources relevant to the Land Soil Cop community.\n\n\nDatasets are either a source for predictions, as well as can be the result of a prediction.\n\n\n\nServices are offered by organizations to facilitate access to data and knowlegde. Services can vary from web services (APIs), SMS services, brochures, up to on site visits.\n\n\n\nPredictions on distribution of environmental parameters in time and space, as well as yield predictions, are calculated using statistical, rule based, and/or machine learning models. Commonly available models are described as resources in the hub.\n\n\n\nIn the hub various software components are described which enable visualisation and/or analysis of relevant data and knowledge sources\n\n\n\nApproaches descrive commonly known mechanisms to improve land management to overcome challenges such as erosion, limited fertility, salinisation, and climate change.\n\n\n\nThe hub describes relevant policies to the Land Soil Crop community. Policy drives the collection of data, but data can also support policy development.\n\n\n\n\n\nSearch for data and knowledge in the hub in various ways:\n\nSearch by keyword or organisation\nOn a search result, further limit the results by filters in the sidebar\n\n\n\n\nHub catalogue\n\n\nWhen you click on submit, the available records will appear.\n\n\n\nHub catalogue overview\n\n\nThe catalogue allows the filtering of keywords. The 3 main keywords are: 1. category ( such as soil, crop, etc.), 2. spatial scope (such as Global, National, district, etc. ), 3. the type (such as dataset, software, etc).\nSecond, any other keyword linked to the resource can be used to search in the catalogue, for example, land use or crop yield. This depends on which keywords are given to the resources.\n\n\n\n\n\n\nTip\n\n\n\nTry the keywords. Type in the search bar various keywords, such as soil or Land use or click on the keywords on the side.\n\n\n\n\n\nHub catalogue search\n\n\nFor each record, a number of metadata properties are provided, such as abstract, used datasets, keywords, usage constraints, and contact information.\nSome records link directly to the map viewer component. Under the image, it will say: Open record in the LSC map and you will be directed to the map viewer.\n\n\n\n\n\n\nTip\n\n\n\nExplore the records. Click, after searching on keywords, on one of the appeared records and explore the provided information. Click on the links in the records\n\n\n\n\n\nHub catalogue record\n\n\n\n\n\n\nSpatial data can be viewed and compared in a web-based map viewer. The map viewer can be accessed on the homepage of the LSC hub, under DATA and then click on Go the map viewer. The map viewer can also directly be accessed at https://maps.lsc-hubs.org/#lsc-rwanda\n\n\n\nHub map vizualisation\n\n\nThe map viewer can used for the visualisation of existing maps listed in the LSC catalogue, your data and web data. In this guideline, we will take you through the available functionalities of the map viewer.\n\n\n\n\n\nmapviewer top bar\n\n\n\nGet more information about the map viewer. The introduction, disclaimer and data attributes are described.\nthe related maps show other available maps, such as the LSC Ethiopia, LSC Keny and LSC Rwanda. If you click on one of these maps, it will zoom to those locations.\n\n\n\n\nmapviewer related maps\n\n\n\nMap settings allows you to select a different base map, such as natural earth maps or aerial maps. This depends on which base map you prefer to work with.\n\n\n\n\nmapviewer map settings\n\n\n\nHelp gives useful tips on how to use the map viewer. It provides a tour through the map viewer and a step-by-step guide. This is an interactive guideline and shows the main functionalities.\n\n\n\n\nmapviewer help\n\n\n\nA Story is a function that allows you to create and share interactive stories directly from your map. It contains a video with an explanation of how to create them.\n\n\n\n\nmapviewer story\n\n\n\nShare/Print generates a link to your created map, which you can share with colleagues. Anything you have added to the map viewer will be shown in the shareable link. You can also use this button to download your created map as an image.\n\n\n\n\nmapviewer share\n\n\n\n\n\nThe vertical toolbar allows you to zoom on the map or location, compare maps, measure distance and provide feedback. Each of the buttons will be explained below.\n\n\n\nmapviewer rightbar\n\n\n\nZoom in and out, and back to a full world zoom\nZoom to your current location\nCompare two map data side-by-side. In the next section on explore map data, we go into more detail on this function.\n\n\n\n\nmapviewer compare\n\n\n\nMeasure the distance on the map between two locations.\n\n\n\n\nmapviewer measure\n\n\n\nProvide feedback on the map viewer. Feedback is essential to improve the map viewer and to ensure that the map viewer fits the user’s needs. If you have any feedback on the map viewer, you can give it by this button.\n\n\n\n\nmapviewer feedback\n\n\n\n\n\n\n\n\nTip\n\n\n\nExplore the basic settings of the mapviewer. Change the map settings, take the tour at the help button, download your current map, measure distances and go to your location.\n\n\n\n\n\nThe sidebar is the main location for adding maps to the map viewer and visualising your data or any other web data.\n\nSearch for locations allows you to search for a specific location and go to your area of interest.\n\n\n\n\nmapviewer location\n\n\n\nExplore map data shows a listing of datasets that can be added to the map via a catalogue search or directly from available maps. If the panel is empty, select an alternative map from related maps.\n\n\n\n\nmapviewer explore\n\n\nUnder the available maps, you can click on a property map to which you would like to add the viewer. It shows, for example, for the property pH, 4 maps: the 5% prediction value, the 95% prediction value, the median of predictions and the pH map.\nThe values are given for pH*10 for better visualisation. Under the data preview, the metadata of the map is given. You can add the map to the map viewer by clicking on Add to the map.\n\n\n\nmapviewer add\n\n\nYou can add as many maps to the Mapviewer as you want. For example, you add another map of Organic Carbon.\n\n\n\nmapviewer add2\n\n\nIf you now click on the compare button as described in the previous section, and put one layer to the left and the other to the right, you can compare the layers side-by-side.\n\n\n\nmapviewer compare2\n\n\nAbout data brings you back to the Explore map Data, and shows you the metadata describing the map. The description gives in addition which datasets are used to generate the maps.\n\nUpload provides the option to open a dataset from the local computer. Note that this data is not uploaded to a server, so this data is not shared with anyone else. You can also add web data from this panel to the map viewer.\n\n\n\n\nmapviewer localweb\n\n\nFor local files, you first need to select a file type. The file should have a spatial component and/or coordinates to add it to the map viewer. In step 2, you browse your file on your local computer.\n\n\n\nmapviewer local\n\n\nFor web data, you first need to select the file or web service type. In step 2, you will add the URL to add the web data. For example, you can add the ESA land cover map as a WMS layer. The URL is: https://worldcover2020.esa.int/geoserver/gwc/service/wms?SERVICE=WMS&VERSION=1.1.1\n\n\n\nmapviewer web\n\n\nYou can compare these maps with other added maps, through the compare button.\n\n\n\nmapviewer compare2\n\n\n\nAs soon as layers are loaded on the map, you can set the order of the layers, view a legend of the layer, zoom to its extent, set its opacity and view the metadata of the data.\n\n\n\n\n\n\n\nTip\n\n\n\nThe steps of this exercise are written down under the help button. By clicking on Take the tour, it will guide you through the steps.\n\n\n\nSearch for a location to quickly find an area of interest\nUse ‘Explore map data’ to view the catalogue of available data sets and add at least two to the map\nInteract with the data layer, including opacity and toggling on and off on the left in your workbench, compare the maps by using the compare button\nClick on the data on the map to view more detailed data, including the raw data\nChange your base map using options in ‘Map Settings’ to help make some data sets more visible\nZoom and change your view, including tilting the view angle using the controls on the right-hand side of the screen\n\n\n\n\n\n\nYou are very much invited to contribute to the development of the hub. The contents of the hub is maintained via a co-creation platform called github.com. You can either directly contribute via the github platform, but a feed back mechanism is also provided on each of the hub resources.\nProviding your feedback is crucial for several reasons:\n\nFirstly, incorporating diverse perspectives from stakeholders involved in working with and benefiting from land, soil, and crop information ensures that the LSC hub caters to the actual needs and demands of its users (Data providers/Users). This feedback allows for the fine-tuning of the hub’s functionalities, making it more user-friendly and effective in serving the specific requirements of different user groups.\nSecondly, gathering feedback facilitates continuous improvement. It provides an opportunity to identify potential shortcomings or areas needing enhancement within the LSC hubs. Insights from stakeholders enable the developers and administrators of the hub to address any challenges faced by users, thereby refining the system to better align with the expectations and requirements of its intended beneficiaries.\nMoreover, involving stakeholders in providing feedback fosters a sense of ownership and collaboration. When users feel heard and their inputs valued, it encourages their active participation and engagement with the LSC hub. This collaborative approach promotes a sense of ownership among stakeholders, leading to increased utilization and sustained support for the system in the long term.\nUltimately, the feedback obtained from diverse stakeholders during the Rwanda WP4 workshop plays a pivotal role in ensuring that the LSC hub evolves as a valuable, user-centric platform, effectively supporting decision-making processes related to land, soil, and crop information in Rwanda’s agricultural landscape.\n\nEvery page or resource on the hub provides an option to provide feedback and/or ask a question related to the content. In these sections, you can provide feedback about the page and what you would like to be adjusted.\n\n\n\nfeedback page\n\n\nContributions to the hub require a Github login. A GIThub account is easily made by pressing on sign in with GIThub, then click on New to GIThub? Create an account.\nYou only need to decide on a username, and password and enter your email address.\n\n\n\nfeedback github\n\n\nOnce logged in, you can now comment below the pages. If you have an account, You can provide feedback by contributing to hub discussions at the github repository. To get started, you can create a new discussion.\n\n\n\nfeedback discussion\n\n\n\n\n\n\n\n\nTip\n\n\n\nLogin to Github or create a Github account and start a discussion about an aspect of the LSC-hub you are surprised about."
  },
  {
    "objectID": "docs/user/index.html#what-is-a-land-soil-crop-hub",
    "href": "docs/user/index.html#what-is-a-land-soil-crop-hub",
    "title": "User Guideline LSC Hubs",
    "section": "",
    "text": "Decision support tools in agronomy heavily depend on acurate environmental and crop data. Many data are available, but often disperse and hard to locate. Land Soil Crop hubs aim to faciliate findability and accessibility of relevant data. Hubs facilitate data and knowledge to be stored, described, processed and visualized.\nThe hubs describe a number of different resources relevant to the Land Soil Cop community.\n\n\nDatasets are either a source for predictions, as well as can be the result of a prediction.\n\n\n\nServices are offered by organizations to facilitate access to data and knowlegde. Services can vary from web services (APIs), SMS services, brochures, up to on site visits.\n\n\n\nPredictions on distribution of environmental parameters in time and space, as well as yield predictions, are calculated using statistical, rule based, and/or machine learning models. Commonly available models are described as resources in the hub.\n\n\n\nIn the hub various software components are described which enable visualisation and/or analysis of relevant data and knowledge sources\n\n\n\nApproaches descrive commonly known mechanisms to improve land management to overcome challenges such as erosion, limited fertility, salinisation, and climate change.\n\n\n\nThe hub describes relevant policies to the Land Soil Crop community. Policy drives the collection of data, but data can also support policy development."
  },
  {
    "objectID": "docs/user/index.html#find-data",
    "href": "docs/user/index.html#find-data",
    "title": "User Guideline LSC Hubs",
    "section": "",
    "text": "Search for data and knowledge in the hub in various ways:\n\nSearch by keyword or organisation\nOn a search result, further limit the results by filters in the sidebar\n\n\n\n\nHub catalogue\n\n\nWhen you click on submit, the available records will appear.\n\n\n\nHub catalogue overview\n\n\nThe catalogue allows the filtering of keywords. The 3 main keywords are: 1. category ( such as soil, crop, etc.), 2. spatial scope (such as Global, National, district, etc. ), 3. the type (such as dataset, software, etc).\nSecond, any other keyword linked to the resource can be used to search in the catalogue, for example, land use or crop yield. This depends on which keywords are given to the resources.\n\n\n\n\n\n\nTip\n\n\n\nTry the keywords. Type in the search bar various keywords, such as soil or Land use or click on the keywords on the side.\n\n\n\n\n\nHub catalogue search\n\n\nFor each record, a number of metadata properties are provided, such as abstract, used datasets, keywords, usage constraints, and contact information.\nSome records link directly to the map viewer component. Under the image, it will say: Open record in the LSC map and you will be directed to the map viewer.\n\n\n\n\n\n\nTip\n\n\n\nExplore the records. Click, after searching on keywords, on one of the appeared records and explore the provided information. Click on the links in the records\n\n\n\n\n\nHub catalogue record"
  },
  {
    "objectID": "docs/user/index.html#map-viewer",
    "href": "docs/user/index.html#map-viewer",
    "title": "User Guideline LSC Hubs",
    "section": "",
    "text": "Spatial data can be viewed and compared in a web-based map viewer. The map viewer can be accessed on the homepage of the LSC hub, under DATA and then click on Go the map viewer. The map viewer can also directly be accessed at https://maps.lsc-hubs.org/#lsc-rwanda\n\n\n\nHub map vizualisation\n\n\nThe map viewer can used for the visualisation of existing maps listed in the LSC catalogue, your data and web data. In this guideline, we will take you through the available functionalities of the map viewer.\n\n\n\n\n\nmapviewer top bar\n\n\n\nGet more information about the map viewer. The introduction, disclaimer and data attributes are described.\nthe related maps show other available maps, such as the LSC Ethiopia, LSC Keny and LSC Rwanda. If you click on one of these maps, it will zoom to those locations.\n\n\n\n\nmapviewer related maps\n\n\n\nMap settings allows you to select a different base map, such as natural earth maps or aerial maps. This depends on which base map you prefer to work with.\n\n\n\n\nmapviewer map settings\n\n\n\nHelp gives useful tips on how to use the map viewer. It provides a tour through the map viewer and a step-by-step guide. This is an interactive guideline and shows the main functionalities.\n\n\n\n\nmapviewer help\n\n\n\nA Story is a function that allows you to create and share interactive stories directly from your map. It contains a video with an explanation of how to create them.\n\n\n\n\nmapviewer story\n\n\n\nShare/Print generates a link to your created map, which you can share with colleagues. Anything you have added to the map viewer will be shown in the shareable link. You can also use this button to download your created map as an image.\n\n\n\n\nmapviewer share\n\n\n\n\n\nThe vertical toolbar allows you to zoom on the map or location, compare maps, measure distance and provide feedback. Each of the buttons will be explained below.\n\n\n\nmapviewer rightbar\n\n\n\nZoom in and out, and back to a full world zoom\nZoom to your current location\nCompare two map data side-by-side. In the next section on explore map data, we go into more detail on this function.\n\n\n\n\nmapviewer compare\n\n\n\nMeasure the distance on the map between two locations.\n\n\n\n\nmapviewer measure\n\n\n\nProvide feedback on the map viewer. Feedback is essential to improve the map viewer and to ensure that the map viewer fits the user’s needs. If you have any feedback on the map viewer, you can give it by this button.\n\n\n\n\nmapviewer feedback\n\n\n\n\n\n\n\n\nTip\n\n\n\nExplore the basic settings of the mapviewer. Change the map settings, take the tour at the help button, download your current map, measure distances and go to your location.\n\n\n\n\n\nThe sidebar is the main location for adding maps to the map viewer and visualising your data or any other web data.\n\nSearch for locations allows you to search for a specific location and go to your area of interest.\n\n\n\n\nmapviewer location\n\n\n\nExplore map data shows a listing of datasets that can be added to the map via a catalogue search or directly from available maps. If the panel is empty, select an alternative map from related maps.\n\n\n\n\nmapviewer explore\n\n\nUnder the available maps, you can click on a property map to which you would like to add the viewer. It shows, for example, for the property pH, 4 maps: the 5% prediction value, the 95% prediction value, the median of predictions and the pH map.\nThe values are given for pH*10 for better visualisation. Under the data preview, the metadata of the map is given. You can add the map to the map viewer by clicking on Add to the map.\n\n\n\nmapviewer add\n\n\nYou can add as many maps to the Mapviewer as you want. For example, you add another map of Organic Carbon.\n\n\n\nmapviewer add2\n\n\nIf you now click on the compare button as described in the previous section, and put one layer to the left and the other to the right, you can compare the layers side-by-side.\n\n\n\nmapviewer compare2\n\n\nAbout data brings you back to the Explore map Data, and shows you the metadata describing the map. The description gives in addition which datasets are used to generate the maps.\n\nUpload provides the option to open a dataset from the local computer. Note that this data is not uploaded to a server, so this data is not shared with anyone else. You can also add web data from this panel to the map viewer.\n\n\n\n\nmapviewer localweb\n\n\nFor local files, you first need to select a file type. The file should have a spatial component and/or coordinates to add it to the map viewer. In step 2, you browse your file on your local computer.\n\n\n\nmapviewer local\n\n\nFor web data, you first need to select the file or web service type. In step 2, you will add the URL to add the web data. For example, you can add the ESA land cover map as a WMS layer. The URL is: https://worldcover2020.esa.int/geoserver/gwc/service/wms?SERVICE=WMS&VERSION=1.1.1\n\n\n\nmapviewer web\n\n\nYou can compare these maps with other added maps, through the compare button.\n\n\n\nmapviewer compare2\n\n\n\nAs soon as layers are loaded on the map, you can set the order of the layers, view a legend of the layer, zoom to its extent, set its opacity and view the metadata of the data.\n\n\n\n\n\n\n\nTip\n\n\n\nThe steps of this exercise are written down under the help button. By clicking on Take the tour, it will guide you through the steps.\n\n\n\nSearch for a location to quickly find an area of interest\nUse ‘Explore map data’ to view the catalogue of available data sets and add at least two to the map\nInteract with the data layer, including opacity and toggling on and off on the left in your workbench, compare the maps by using the compare button\nClick on the data on the map to view more detailed data, including the raw data\nChange your base map using options in ‘Map Settings’ to help make some data sets more visible\nZoom and change your view, including tilting the view angle using the controls on the right-hand side of the screen"
  },
  {
    "objectID": "docs/user/index.html#hub-community",
    "href": "docs/user/index.html#hub-community",
    "title": "User Guideline LSC Hubs",
    "section": "",
    "text": "You are very much invited to contribute to the development of the hub. The contents of the hub is maintained via a co-creation platform called github.com. You can either directly contribute via the github platform, but a feed back mechanism is also provided on each of the hub resources.\nProviding your feedback is crucial for several reasons:\n\nFirstly, incorporating diverse perspectives from stakeholders involved in working with and benefiting from land, soil, and crop information ensures that the LSC hub caters to the actual needs and demands of its users (Data providers/Users). This feedback allows for the fine-tuning of the hub’s functionalities, making it more user-friendly and effective in serving the specific requirements of different user groups.\nSecondly, gathering feedback facilitates continuous improvement. It provides an opportunity to identify potential shortcomings or areas needing enhancement within the LSC hubs. Insights from stakeholders enable the developers and administrators of the hub to address any challenges faced by users, thereby refining the system to better align with the expectations and requirements of its intended beneficiaries.\nMoreover, involving stakeholders in providing feedback fosters a sense of ownership and collaboration. When users feel heard and their inputs valued, it encourages their active participation and engagement with the LSC hub. This collaborative approach promotes a sense of ownership among stakeholders, leading to increased utilization and sustained support for the system in the long term.\nUltimately, the feedback obtained from diverse stakeholders during the Rwanda WP4 workshop plays a pivotal role in ensuring that the LSC hub evolves as a valuable, user-centric platform, effectively supporting decision-making processes related to land, soil, and crop information in Rwanda’s agricultural landscape.\n\nEvery page or resource on the hub provides an option to provide feedback and/or ask a question related to the content. In these sections, you can provide feedback about the page and what you would like to be adjusted.\n\n\n\nfeedback page\n\n\nContributions to the hub require a Github login. A GIThub account is easily made by pressing on sign in with GIThub, then click on New to GIThub? Create an account.\nYou only need to decide on a username, and password and enter your email address.\n\n\n\nfeedback github\n\n\nOnce logged in, you can now comment below the pages. If you have an account, You can provide feedback by contributing to hub discussions at the github repository. To get started, you can create a new discussion.\n\n\n\nfeedback discussion\n\n\n\n\n\n\n\n\nTip\n\n\n\nLogin to Github or create a Github account and start a discussion about an aspect of the LSC-hub you are surprised about."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.1-overview.html#agenda",
    "href": "docs/developer/tutorial-data-management/slides/1.1-overview.html#agenda",
    "title": "1.1 Training Overview",
    "section": "Agenda",
    "text": "Agenda\n\n\n\nDay\nTopic\n\n\n\n\nDay 1\nFAIR principlesMetadata management\n\n\nDay 2\nMetadata publication\n\n\nDay 3\nData publicationQuality of Service\n\n\nDay 4\nCases\n\n\n\n\n\n\n\nWorkshop on effective spatial data management"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.3-tools.html#github",
    "href": "docs/developer/tutorial-data-management/slides/1.3-tools.html#github",
    "title": "1.3 Tools",
    "section": "Github",
    "text": "Github\nService to facilitate co-creation\n\nversion history of contributions\nauthentication\nissue management\nrelease management\nContinuous integration\n\nRelated software; Git, Gitlab, Bitbucket, Codeberg"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.3-tools.html#quarto",
    "href": "docs/developer/tutorial-data-management/slides/1.3-tools.html#quarto",
    "title": "1.3 Tools",
    "section": "Quarto",
    "text": "Quarto\nA content management system to create websites, documentation, slides, etc.\nRelated software; Hugo, mkdocs, Jekyll, Wordpress, Drupal"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.3-tools.html#pygeometa-geodatacrawler",
    "href": "docs/developer/tutorial-data-management/slides/1.3-tools.html#pygeometa-geodatacrawler",
    "title": "1.3 Tools",
    "section": "pygeometa / geodatacrawler",
    "text": "pygeometa / geodatacrawler\nScripting libraries which support data management of a file repository\n\nGenerate metadata from existing data\nImport remote metadata\nIngest metadata from a file repository\nCreate mapservices (APIs) for existing data"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.3-tools.html#pycsw",
    "href": "docs/developer/tutorial-data-management/slides/1.3-tools.html#pycsw",
    "title": "1.3 Tools",
    "section": "pycsw",
    "text": "pycsw\nA catalogue server application providing support for multiple metadata schemes and metadata exchange standards\nRelated products; GeoNetwork, CKAN, Dataverse"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.3-tools.html#mapserver",
    "href": "docs/developer/tutorial-data-management/slides/1.3-tools.html#mapserver",
    "title": "1.3 Tools",
    "section": "Mapserver",
    "text": "Mapserver\nA server application providing OGC services (WMS, WFS, WCS) on various data formats\nRelated products; Geoserver, ArcGIS server, deegree, pygeoapi, QGIS server"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/1.3-tools.html#terriajs",
    "href": "docs/developer/tutorial-data-management/slides/1.3-tools.html#terriajs",
    "title": "1.3 Tools",
    "section": "TerriaJS",
    "text": "TerriaJS\nA WebGIS application including Leaflet library\ndemo\nRelated products; GeoNode, GeoMoose, MapBender, Oskari, GisQuick\n\n\n\n\nWorkshop on effective spatial data management"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#what-is-ogc",
    "href": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#what-is-ogc",
    "title": "3.1 Data services",
    "section": "What is OGC?",
    "text": "What is OGC?\n\nOpen Geospatial Consortium defines standards for the geospatial industry\nMembers are software companies, universities and government\nSuch as WMS, GML, OGCAPI, EPSG, GeoSPARQL\nRead more at https://opengeospatial.org"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#ows-vs-ogc-api",
    "href": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#ows-vs-ogc-api",
    "title": "3.1 Data services",
    "section": "OWS vs OGC API",
    "text": "OWS vs OGC API\n\nThe GML and OWS standards are defined on 20**\nSince then the internet moved away from xml\nin 2017 started spatial data on the web experiment with W3C, leading to OGC API\nCurrently under development\nBasic support in new and existing products\nThe browser as a tool to browse spatial data"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#ows-vs-ogc-api-1",
    "href": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#ows-vs-ogc-api-1",
    "title": "3.1 Data services",
    "section": "OWS vs OGC API",
    "text": "OWS vs OGC API\n\nGML - GeoJSON (json-fg) & HTML\nLong querystrings - Rest api (path parameters)\nGetCapabilities - Open API Specification\nFormat=xxx - Content negotiation (accept:text/html)\nFull featured - Minimal standard with extensions"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#wfs-and-ogc-api---features",
    "href": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#wfs-and-ogc-api---features",
    "title": "3.1 Data services",
    "section": "WFS and OGC API - Features",
    "text": "WFS and OGC API - Features\n\nProvides access to vector features.\nFilter, sorting and pagination options\n\n\nwms"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#wms-and-ogc-api---maps",
    "href": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#wms-and-ogc-api---maps",
    "title": "3.1 Data services",
    "section": "WMS and OGC API - Maps",
    "text": "WMS and OGC API - Maps\n\nImage rendering of subsets of the data using some styling rules\n\n\nwms"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#wmts-tms-and-ogc-api---tiles",
    "href": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#wmts-tms-and-ogc-api---tiles",
    "title": "3.1 Data services",
    "section": "WMTS, TMS and OGC API - Tiles",
    "text": "WMTS, TMS and OGC API - Tiles\n\nImage or vector representation in tiles on a grid\n\n\nwms"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#wcs-and-ogc-api---coverages",
    "href": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#wcs-and-ogc-api---coverages",
    "title": "3.1 Data services",
    "section": "WCS and OGC API - Coverages",
    "text": "WCS and OGC API - Coverages\n\nGrid extracts and calculations\n\n\nwms"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#csw-ogc-api---records-and-stac",
    "href": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#csw-ogc-api---records-and-stac",
    "title": "3.1 Data services",
    "section": "CSW, OGC API - Records and STAC",
    "text": "CSW, OGC API - Records and STAC\n\nCatalogue records\nSTAC and OGC API - Records both based on OGC API - Features"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#mapserver",
    "href": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#mapserver",
    "title": "3.1 Data services",
    "section": "Mapserver",
    "text": "Mapserver\n\nRecent versions of Mapserver support the old and new standards\nFeatures and Maps\nNo tiles (mapproxy), No coverages yet\nNo records/stac, by design"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#mapfile",
    "href": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#mapfile",
    "title": "3.1 Data services",
    "section": "Mapfile",
    "text": "Mapfile\n\nConfiguration via a mapfile\nAll gdal formats supported\nCreate mapfiles using QGIS with geocat bridge\n\n\nwms"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#sensorthings-api",
    "href": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#sensorthings-api",
    "title": "3.1 Data services",
    "section": "Sensorthings API",
    "text": "Sensorthings API\n\nSoil profile data is observation oriented (iso28258)\nSensorthings API (STA) is an OGC specification for sensor data exchange\nFrost server is an implementation of STA\nLimited availability of clients for STA, Qgis has a STA plugin"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#cloud-optimised",
    "href": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#cloud-optimised",
    "title": "3.1 Data services",
    "section": "Cloud optimised",
    "text": "Cloud optimised\n\nRecent interest in cloud optimised and cloud native formats\nLarge data files are places in a cloud bucket, users request subsets of the file using range requests\nFor grids, Cloud Optimised GeoTiff (COG) and GeoZarr (cloud native)\nFor vector, GeoParquet"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#sematic-web",
    "href": "docs/developer/tutorial-data-management/slides/3.1-ogc-data-services.html#sematic-web",
    "title": "3.1 Data services",
    "section": "Sematic web",
    "text": "Sematic web\n\nTo integrate spatial data in the web of knowledge, semantic web standards are relevant\nDCAT for metadata\nGeoSPARQL to define geometries\nSOSA for sensor data\nGLOSIS Web Ontology is a soil ontology based on SOSA and GeoSPARQL\n\n\n\n\n\nWorkshop on effective spatial data management"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#history",
    "href": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#history",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "History",
    "text": "History\nVarious efforts have been made to harmonise Soil Profile data\n\nGlobalSoilMap\ne-SOTER\nISO28258\nGLOSIS Web ontology"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#e-soter",
    "href": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#e-soter",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "e-SOTER",
    "text": "e-SOTER\n\nRelational database model, usually in Microsoft Access\nKenya\nSouthern Africa\nSenegal and Gambia"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#ken-soter",
    "href": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#ken-soter",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "KEN-SOTER",
    "text": "KEN-SOTER\n\n\n\n\n\nKEN SOTER map\n\n\n\n\n\n\nKEN SOTER model"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#iso28258",
    "href": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#iso28258",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "ISO28258",
    "text": "ISO28258\n\n\n\nISO28258 is based on Observations and measurements\nevery field observation and laboratory analysis as a sensor observation\n\n\n\n\n\nO&M model"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#iso28258-in-gml",
    "href": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#iso28258-in-gml",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "ISO28258 in GML",
    "text": "ISO28258 in GML\n\nSerialisation of ISO28258 in GML (app schema)\nQuite complex implementation and low support in tooling\nINSPIRE Soil (Europe)\n\nANZSoilML (Australia)\nAlternative is a serialisation in JSON using Sensorthings API."
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#iso28258-as-relational-database",
    "href": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#iso28258-as-relational-database",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "ISO28258 as relational database",
    "text": "ISO28258 as relational database\n\nA relational model is available according to ISO28258\nEither as PostGreSQL or GeoPackage (SQLite)\nRead more at https://iso28258.isric.org"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#glosis-web-ontology",
    "href": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#glosis-web-ontology",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "GLOSIS Web Ontology",
    "text": "GLOSIS Web Ontology\n\nGLOSIS Web Ontology has expressed ISO28258 using common web ontologys (SOSA, VCARD)\nIncludes codelists for soil properties and procedures"
  },
  {
    "objectID": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#harmonisation-of-field-lab-and-legacy-data",
    "href": "docs/developer/tutorial-data-management/slides/4.1-soil-profile-data-management.html#harmonisation-of-field-lab-and-legacy-data",
    "title": "4.1 Soil Profile Datamanagement",
    "section": "Harmonisation of field, lab and legacy data",
    "text": "Harmonisation of field, lab and legacy data\n\nUse common codelists to identify properties, methods, units\nHale Studio and FME are tools for data harmonisation\nAlso SQL scripts are often used to harmonise data\nSemantic web community offers harmonisation tools (rml, tarql, csv-ld)\n\n\n\n\n\nWorkshop on effective spatial data management"
  }
]